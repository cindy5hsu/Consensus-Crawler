 Anna, the creator of Vana Protocol, which is a way for users to own their data. Just to get a sense, are people coming from an AI background or crypto background? Okay. Maybe more crypto. Okay. Cool. So I will be talking a little bit from kind of an AI research perspective about the role that data plays in AI. And then I will be going into some of the inner workings of how the Vana Protocol works. Some of the interesting mechanisms we will talk about are data tokens, which basically act as this way to tokenize a specific data set. So AI models are ultimately really only as good as their training data. So if you've posted on Reddit, you've already helped to contribute to AI models. This data set is basically showing kind of what GPT-3 and a lot of the large language models are trained on today. But it represents a very small percent of the overall internet. So people estimate that that's less than like 0.1% of all of the data that's available. The reason why this matters is that if you want to build better AI, you need access to more and more data. So if you're following kind of in the AI research community today, there's a discussion of the data wall. What the data wall refers to is the fact that we're actually running out of data to train AI models on. So some of the leading AI models today, like Llama 3, are trained on around 15 trillion tokens, which is like a word. It's not like crypto token. And that's actually the size of the public internet. So we've essentially run out of data to train AI on. And if we want to make it better, we need to find some place to get that data. The data wall is an issue of both quality and quantity. I don't know if people follow DeepSeq, the AI model that came out recently that was really high quality. One of the things that a lot of people don't know about DeepSeq is that it actually overcame the data wall through quality. So what they did is they hired a team of PhDs to generate really high quality data. So they took these highly educated people and had them generate like math and coding data that they then used to kind of bootstrap the data set. If you want like a human analogy, you can think about an AI as like a kid that's learning about the world and you're giving it data to teach it better and better. So what DeepSeq did is they gave it really high quality textbooks. They got it really good teachers. And that's why they were able to train it so efficiently. Companies have realized that this data piece is really important and really valuable, right? So Reddit is earning $200 million from selling data for AI training. Photo Bucket's earning $1 to $2 an image. On the buy side, Apple is buying data for $50 million. My co-founder Art used to actually work selling data to big tech companies. And a lot of these contracts are not that public, but extremely large because you need this data to be able to train better models. As companies have realized my data is really valuable, what they've done is actually made the access to the data harder for other people to get. So like Stack Overflow and Reddit have changed their API policies saying, hey, nobody can access our data except us or if you pay us to train AI models on it. There's one group that still has access to their data, which a lot of people don't realize. So users legally own all of their data, right? If you have posted on a platform or have sent a message, that's legally yours. In the same way that when you park your car in a parking lot, the parking lot doesn't own your car, right? It's still your car. They can't just randomly take it for parts. That's the same sort of legal framework with your data. Most users don't realize this. And most companies that have your data don't necessarily want you to realize this either. But you can always go to a platform and get a full copy of your data back. So like if you use Instagram, you can just Google like Instagram data export. They'll give you everything. They'll give you your images, your posts. They'll also give you the AI labels that they put onto you. So one of the AI labels that they had put onto me is stressed but not depressed. And it's actually like shocking what they have on you. So it's just a fun activity to go get your data. So what this allows for is basically cutting through the walled gardens, right? A lot of people today, when you think about the data sets that companies have, they're living inside of the walled gardens. But each user can go and take their own data back. So VANA is the self-sovereign data layer for AI. In terms of the core concepts that VANA runs on, the first is non-custodial data. So what VANA allows you to do is to manage access to your data just like your funds. So you sign a transaction and that is basically granting access to your data. The second piece is proof of contribution. So if all of us like exported our Telegram messages and pulled them together in some kind of privacy preserving way, the risk would be that one person could pretend to be like 100 people and then take control of the data set. So what proof of contribution does is it ensures that when a group of people is pooling their data, it is being verified and meets a quality standard. There are a bunch of different data dials across the ecosystem. So here are some of the projects. I'll call out a few. YKYR is a Chrome extension where they're basically collecting browsing data and trying to build decentralized Google Analytics. The auto DLP is started by a guy who used to work selling data in Web 2 and then saw the opportunity to kind of start these sorts of models. They're one of the first DLPs who's already commercializing a lot of their data set. So if you have a Tesla, you can earn from auto DLP. I think it's like $3 a month per person or something like that. And the buyer on that side is actually using it to understand how car batteries degrade. So that's just one example. Nowhere is a Google location data set. Each of these is their own team. Typically, a data dial team is maybe three to five people. We have an accelerator program for the different data dials helping people learn how to build valuable businesses around data. Something I'll note, too, is that each data dial has its own data token and proof of contribution. So they have tokenomics designed specifically for their data set. A few of the data dials to highlight, one of them is the Reddit data dial. That's one of the largest ones. There are 140,000 users who contributed their Reddit data into that one. And now they've trained a user-owned large language model. That large language model, because it's mostly trained on, like, Reddit messages and comments, people have been joking that it's just good at, like, shitposting. But it is kind of that foundation of being able to take data from a large group and then create a user-owned model. The other one I would call out is the 23andMe DAO. So if people are familiar with 23andMe, the company, basically you, like, mail in your DNA and then they give you kind of information about what you're like. And usually people don't do it for the purposes of giving 23andMe their data. And that kind of wasn't why people had signed up. But 23andMe is kind of struggling from a financial perspective. And so now they're kind of sitting, I think, around $75 million market cap at this point. And they might have to sell to a company that wants to buy that data set. So what the 23andMe DAO is trying to do is actually buy 23andMe outright and make it user-owned. The goal is not to sell the data set, but actually to block the sale of the data. So in terms of what data allows for, Vana allows for, is really keeping data private yet interoperable. So one of the challenges with working with data is what we call the double spend problem for data. So if I give you my data, how do I know that you haven't, like, taken a copy of my data and used it, right? So what Vana allows for is running operations on a TEE on that data, both at the individual level or at the aggregate level, but only letting the outputs leave unencrypted, right? And this is a really important core primitive because without it, you cannot form a data market, right? If you have a place where, like, the economics can leak, so if I can take a copy of your data or I make it public in some way, then I have no reason to pay you for that data. So that privacy piece is extremely important. From a technical perspective, Vana is basically an L1 that can work with private data. Usually blockchains can't do that because everything is public. And so that's kind of the core primitive that we've added to turn data into an asset class. Something else I'll mention about data on Vana is that it's structured and quality checked. So each data DAO is basically defining their data schema. Here I have the data schema from sleep.fun where you can connect your sleep data in. And what's nice about that is you know the format of data coming in so any developer can build on top of it. One application I think could be really cool to see with sleep.fun data is if you tied someone's on-chain trading data or, like, Coinbase data, kind of token-holding data to their sleep, you could basically track, like, how well are token holders of different projects sleeping. And that's a sort of application that is only possible with access to these sorts of data sets. What Vana does is really unlock data as a new asset class, right? I think one analogy here is a lot of companies that buy oil, they buy oil futures, right? Because it's such an important input to what they do. What we're starting to see is people buying, like, compute futures. That's relatively early. But, I mean, if you're burning, like, yeah, some of the AI companies burn a ton of money on compute. So it makes sense. And I think that data is that natural next step of this asset class, right? So you can have these different data primitives that are possible because you have this tokenization of data. In terms of the mechanics, like, I feel like the idea of kind of, like, data tokens sounds good at a high level, but often the details are very important. So I'll actually walk through how a data token works. Each data set has its own data set specific token, which is created through a DLP. And a DLP stands for data liquidity pool. That's a primitive on Vana, which informally is known as a data DAO. So if you're already deep in Vana, you might know about that. And then when we contribute our data, so, like, if I add my telegram data in or email data in, I get back tokens in return after it's verified. The number of data tokens I get back depends on the quality of my data, right? So in the case of Reddit, that one is based on the square root of your karma. If you have more karma, you have plausibly more valuable data. Some of the other ones reward everyone equally. Then when someone is accessing data, they buy and burn data tokens for access. On Vana, it's done, like, 80% in the data token and 20% in the Vana native token for data access. The other thing that I'll call out about data tokens is that you can build all sorts of data 5 primitives on top of these data tokens, right? So there are a couple of DEXs that basically let you trade these different data set tokens. There's Flora.sh and Datadex on Vana. And then there's even a way for AI agents to actually trade data. So someone recently built, like, an Eliza OS plugin where those AI agents can buy their own data and trade it. Something else I'll mention about Vana is the DLP incentives, which are designed to accelerate data onboarding. So the top 16 DLPs by stake earn incentives for onboarding data. Actually, are people familiar with the Tau subnet tokenomics to some extent? Okay, nice. So, yeah, if you already know that, it's actually somewhat similar. But essentially what you do is you have the top 16 data DAOs that are competing on a set of performance metrics in order to earn a share of emissions, right? And so if you have a lot of data access volume, data quality value, or amount state, then you're able to kind of earn a share of those emissions. In terms of where the other side of that comes from, it's the fact that our bet is that emission is worth the fee long term for the data being accessed in the system. One upcoming change I would call out is that right now the incentives go directly to the DLP creators. In an upgrade that is coming in Q2, there will be an automatic buy and burn mechanism to the data tokens, right? So essentially what that does is route those rewards into the data tokens for data contributors and anyone else who has participated. I added a few other data DAOs here. So one of them is MindDow. They're focused on a mental health data set. One of them, too, that's kind of interesting is Finquarium, where they have you connect in your centralized exchange trading history. This one has been interesting where I think they support right now Coinbase and Binance. So you can get kind of like trading data from those platforms that usually just stays within them. They've seen interest from other centralized exchanges on basically like understanding what is going on in their competitors. So that's been an interesting dynamic to track. In terms of kind of getting started on Vana, this is pretty builder focused. But what you're able to do is kind of pick a data source. So you don't need anybody's permission to start a data DAO. Because users own their data, you don't have to go ask Spotify, hey, can I start a Spotify data? Data DAO, because the user is just getting their own data. Figure out how users will contribute. So every data DAO builds their own UI. In some cases, it's a Chrome extension. In some cases, it's a Telegram bot or kind of a web UI where you're contributing your data. There's a set of template smart contracts to deploy. The proof of contribution is implemented in Python. And then what you're able to do is kind of launch, register your data DAO, and compete to get into the top 16. As I mentioned, like most of the teams are a few people. It takes a few weeks to build out. And you can create like a template data DAO in just a few days. So this is a relatively builder focused view of Vana. But happy to take any questions people might have. Okay. I'll ask one question that I get sometimes is like, how does private data work on Vana? Like how are you actually putting, using a blockchain to interact with private data? The reason why that privacy piece is important is that if my data becomes public, I then can't charge money for it. Right? You've kind of lost that privacy. It's actually an economic question as well as a user adoption question, which is I don't want to like put my iMessages somewhere where they could become public. So what Vana uses for this is a set of TEEs. And the data is only ever accessed inside of the TEEs. So the only place where you work with unencrypted data is in those TEEs. So that's both at the individual level, like accessing my data, but also at the data DAO level. Yeah, that is like a pretty technical view. And I think this might be a little too technical, but I appreciate, yeah, you all joining today. So any other? Yeah, go ahead. Hi. So yes, you are provided somehow tools to build DAOs as well. So the core of the DAO is usually an incentive mechanism because you should incentivize people to participate in the DAO. If I want to use a DAO and I'm a general, you know, an individual who is not familiar with anything, do you provide a basic incentive mechanism that I can use as core of my DAO as well? Yeah. Yeah. So what you're able to do as an end user participating in a data DAO is contribute your data and then get back a data token proportional to that. And as a user, you can choose to sell that data token if you just want kind of a one-off relationship, or you can choose to hold on to it if you want to be part of the DAO. I think that one thing I've seen in DAOs is that often, just more broadly than just data DAOs, you have a huge amount of attention comes towards a DAO, right? And a bunch of people get involved. And then a few months later, everyone kind of forgets about it, right? Which is like not working that well in a DAO context. But actually with a data DAO, because you have a large group of people who come in and contributed their data, even if you have a small group that stays around and is managing the data set, you're still able to have a really functioning data DAO. Thanks for the question.