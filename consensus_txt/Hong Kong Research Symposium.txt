 This is Alan Au from the Hong Kong Polytechnic University. I am a professor and also the associate head of the Department of Computing and also the director of the PolyU Cyberverse Academy, joined up on law and web3. So it's really my honour to be on stage. And today we have the Hong Kong Research Symposium hosted by me and also my colleague Professor Daniel Law, who will be here shortly. So on behalf of the Hong Kong Polytechnic University, I would like to welcome you all to the Hong Kong Research Symposium at the Consensus Hong Kong 2025. And this symposium is hosted by the Research Centre of Blockchain Technology of PolyU and in support of the Law and Web3JR Lab of PolyU. So we are honoured to host this symposium as part of Consensus Hong Kong, one of the most influential global events in the community. And with the local government's vision of making Hong Kong into a leading web3 hub, the Hong Kong Polytechnic University is proud to be one of the forefront of the academic institution to drive this transformation. And we are aimed to act as a bridge between the academia and the industry. And we are really happy to see a lot of research outcome bringing to the real world application. And today we are privileged to have a line-up of distinguished speakers and panellists, including Professor Jin Tai Ding, Professor Marie Mordok, Professor Jian Long Chao, and also Dr. C. C. Duan, Dr. Li Yi and Dr. Patrick McCurry. So I'm sure their insightful talks and panel discussion will give us a lot of new things to learn and inspiring new innovations. So without further ado, thank you everyone and let's have a quick start. I guess now it's time to introduce... Let me introduce our first keynote speaker, Professor Jin Tai Ding. Professor Ding is a renowned expert in post-quantum cryptography. He is one of the designers of the only post-quantum key establishment protocol which has been standardised by NIST. That one is formally known as Kaba. And he is currently the Dean of the School of Mathematics and Physics at the of Xi'an Jiao Tong, Liverpool University, and a child-type distinguished professor amorestist at the University of Cincinnati. So let's welcome Professor Ding. Hello? Hello? Yeah. Okay, okay. Okay. Okay, good afternoon. So my name is Jin Tai Ding. So first I would like to thank Politech U Consensus to give me the opportunity to speak here, in particular my friend Alan. Okay? So again, my name is Jin Tai Ding. Now I'm a professor at Xi'an Jiao Tong, Liverpool University. And the title of my talk is Post-Quantum Cryptography, Migration, A Historical Challenge and Opportunity. In some way, John Lilic from Telos earlier prepared a little bit for me already. So let me now start the talk. So how many of you have seen this number? Does anybody know what this number is? Okay. Okay. So this number is a number such that it's a product of two prime numbers. Okay. Nobody knows except people in Microsoft. Okay. Why this number is important? Because this is a number Microsoft use for Microsoft XP update, software update. Okay? And we all know if you run Microsoft system, often you have to update your operating system. So what does it mean by update your operating system? Which means you are running a new version of the software. So how do you know it's from Microsoft? How do you know there's no virus inside? Somebody modify it. It's because Microsoft use that number, okay, as a public key. Which means that is known to all of you. You can use that number to verify Microsoft what? Made this software. And only Microsoft who knows the factor can sign it. So that's how it works. Okay? So this is the basic idea behind. And this is what is called a digital signature. Okay? Let me repeat. So we have two sets of numbers. One is P and Q. Two prime numbers. And then one number is N. Okay? N is a product of the two numbers. So N is public. Everybody, anybody can use to verify a signature. And P, Q is private. Only Microsoft can use to sign a document. And if you modify anything, you will know. You will reject it. So this is how we build trust of the whole internet. Okay? In human history, it never exists such a thing before. Never. No. If I sign my document, right? I think anybody can imitate my signature. Because my signature is too simple. But even in that case, can you verify some of the signature we can edit? Any present can you verify? No. No. And you can imagine, okay? You can imagine if you are a smart guy who can factor that number, what can you do? You can what? You can pretend to be Microsoft, send a software update, put a virus inside saying delete hard drive. Then all the hard drive will delete. The whole world will collapse tomorrow. So we are living in a new era, very new era. I would say in the God of math we trust. So at this moment, okay, the whole internet security depends on public cryptography. Okay? We trust mathematics as like a God. So this is where we are now. Okay? And why do we do this? So this is, in order to explain the story, I have to explain the difference between public cryptography and the traditional cryptography. Traditional cryptography is what is called a symmetric crypto. For example, this Enigma machine. Okay? But in order to use this machine, the sender and receiver must have exactly the same key. That's why it's called what? Symmetric. Therefore, in order to use it, you must do something called prior key exchange, which means we have to meet and we have to agree on our keys. Okay? So this is the example, right? Right now we are also using symmetric cryptography for you on the AES. The advanced encryption made by the US government again. Okay? The most important thing, such a solution is not scalable. For example, you notice that Enigma machine is only used by German submarines. Why not use it in the army? Why not? Because they all share the same key book. If you distribute to the army, you have to distribute too many copies. If one copy is lost, the whole thing what? Collapse. So therefore what? They only use the submarines. The submarine has one good thing. If they are defeated, what happened to the key book? It goes down to the bottom of the ocean. So that's why this kind of thing is not scalable. Let me tell you one more interesting story. We all know the system was broken by Turing in 1942. But we didn't hear about until 1990. For example, in the Falkland Island War, the Argentinian Navy still use Enigma machine to do communications. This is the reason why the British could thank all the Argentinian warships. Because they know exactly where they are. They know exactly their route. Why? Because the British government kept the fact that they broke the Enigma machine for secret for 50 years. And they even were willing to sacrifice Turing. We know Turing died because he was homosexual. But Turing was not allowed to tell the people, government, what? I did this. So therefore he had to be punished. And recently, King Elizabeth apologized. So this is what happened. And then, this is Turing. There's a reason I tell this story just a little bit later. And then, this is okay. Before that, you know, this is a symmetric key used by government and so on. No problem. Just as long as you don't scale it up. But then in 1960, 70, something happened. In 1967, we started to build what is called large computer network. Now we have billions of devices. We all have to talk to each other. We have to talk to your security. Okay? So therefore, in order to build trust, in order to build security on large computer network, we must find efficient solution. Okay? Imagine, if we don't have a public recovery in China, we have 1.4 billion people. Okay? That means I have to meet every single one in our token security to do a key exchange. And my whole life, we'll do what? A key exchange. We'll not do anything else. So therefore, with a critical moment in 1960, 70s, we start to think about it. Okay? So who gave the solution? The person who first proposed the idea of public key cryptography was by Diffie and Hellman. These are the two guys on top. They proposed the idea of public key cryptography. It's all called asymmetric crypto, which means we should have two sets of keys instead of the same keys. One is public, announced to the whole world. Another one is private. That can solve all the problems on the internet. Okay? And the first people who solve this problem are RSA, Rivest, Shamir, Edelman. They all run Turing prices. Okay? And the example I gave earlier is called the RSA scheme and proposed by the three guys. So let me repeat. They are based on mathematics. They are based on the following facts. You know the number N, but it's very difficult to find this factor. Okay? So this is what happened. Okay? So this is it. But one more thing you should remember. This cryptography is very good. For example, my public key, as long as I launched once, anybody can send me a message. Anyone. So therefore, it's a solution one to many, not one to one anymore. And it's also transitive. Because if it's my public key, you give it to somebody else, it doesn't matter. They can use it. Okay? So this is very important. This is a very revolutionary solution. Okay? Public key cryptography. Okay? So now we have two things. Okay? Digital signature. Let me repeat. Right? Microsoft update. They have what? Public key? Used to verify. Private key? Used to sign. Okay? And then for communications, we do the same. I have a public key. Anybody can send me a message. Only I, who knows the security, can decrypt. And now at the moment we use SSL, TLS and so on. This is the reason you dare to send your credit card over the network. This is protected by public key cryptography. Okay? So this is what happened. Okay? Okay, now come to Bitcoin. So Bitcoin, why Bitcoin is called crypto? For me, I'm a cryptographer. I'm a little bit upset. When I say crypto, I meant cryptography. But now people say crypto means cryptocurrency. Well, a little bit upset. But what can I do, right? So let's look at Bitcoin. How does it work? The Bitcoin is basically based on two algorithms. Number one, ECDSA. Elite Curved Digital Signature. So this is again, a signature scheme. Okay? What is used for? It's used for property rights. Okay? You say you own some Bitcoins. No, you don't. It's the private key owner what? Owns the what? The Bitcoin. Okay? And the Bitcoin is attached to the public key. But the real ownership, where is the real ownership? It's the what? Private key. If you lose the private key, you're gone. You cannot like a bank. Go bank, show your ID. I lost my bank keys. Okay, here you cannot. Okay? Yeah, so this is very important. So we use digital signature. The digital signature is what? For claim the rights. Another thing we use what? This hash function for mining. What's that for? This is for what? For synchronization. Because Bitcoin is what? It's decentralized. We all have a copy. But at the end of the day, we have to what? We have to synchronize. Okay? So that's what is used. So therefore, we use two things. Let me repeat. Hash function for proof of work. And then Elipkib Digital for the rights. Okay? This is what happened. Yeah? There's no encryption involved. In Bitcoin, there's no encryption. Okay? Yeah? So this is what happened. Yeah? So let me repeat. Okay? So what is the, what we would, this is an age of digital economy. And you should realize the security foundation of a digital economy of the whole internet relies on public cryptography. Reliant on the fact that we believe certain mathematical problems are impossible or hard to crack. Okay? So that's it. Okay? Yeah? So the characteristic is one too many. Once I announce my public key, anybody can verify I, what, sign something. Okay? And we don't care about one-to-one solution. Not at all. Okay? So this is what happened. Okay? So at the moment, in the whole world, there are two very important mathematical problems. Number one, integer factorization. Number two, discrete log. If any of you can solve this through mathematical problem, you are more powerful than Trump. You can go to any bank account, the change you what? Add a zero to your account. No problem. So this is where we are now. Okay? This is it. Okay? Yeah? So you will be the king of the world if you can what? Solve this through mathematical problem. So what is the problem? So earlier, the John Lillick, earlier, gave an explanation. Probably some of you saw this picture. What is this picture? Anybody saw it before, right? This is the picture attached to Willow. Google announced the Willow. So what is it? It's a quantum computer. Okay? And the idea of a quantum computer was a quantum computing was proposed long ago. It's almost the same time as public cryptography. One of the main guys proposed the idea is called Richard Feynman. I think you guys know. He's a Nobel Prize winner and he's a very interesting guy. Yes? And this is a picture taken for the investigation, I think, on a challenge explosion. You know? And this is a very... So what is quantum computing? The basic idea is we would like to use quantum mechanics principle to do computing. Okay? It was produced in 1970s, 80s. But we didn't do anything. Nobody did anything because we don't know what it's for. We don't know what a problem it can solve. Until 1995, comes another guy. He said, I mean, it's Peter Shaw. He's now a professor at MIT. What did he do? He found a new quantum algorithm that can solve integer facultation and discrete log problem. The two problems I just mentioned efficiently. But under one assumption, you must have a large scale quantum computer. Okay? So therefore, we don't know if you can build a quantum computer. Nobody cares. For me, something happened and changed my life. So I'm a mathematician. I do algebra. Okay? And then in year 2000, I saw news about this guy. His name is Isaac Truong, who is also a professor at MIT now. You know what he did? He spent 15 million US dollars to show 15 equal to 3 times 5. I said, we can all do it. Do you agree? No. He what? Look at this device. He built a 7 qubits quantum computer and practiced Shaw's algorithm on it. And factor 15 equal to 3 times 5. So what is it? This is called a proof of concept. What he proved is that if we can scale up the quantum computer, we can what? Efficiently run Shaw's algorithm, solve discrete log, and what? Factorization. And what? We'll destroy the whole digital economy. Okay? So this is what happened. Okay? The problem is just scaling. Okay? So mathematicians react to it. What we would like to do to develop a new system that can resist quantum computer attacks. This is called the post-quantum cryptography. Which means a new cryptography can resist quantum computer attacks. Earlier, John said something about quantum cryptography. Please do not confuse these two words. Quantum cryptography means use physics to do cryptography. For me, that's totally different. Irrelevant. Okay? It's not cryptography in my opinion. Okay? So this is what happened. Okay? So I started working in the year 2000. Okay? I was doing algebra. I gave up. I started working in the area. At the beginning, nobody paid attention. And then things started to pick up. And the fundamental thing, event happened is the 19th of August 2015. A friend called me in Germany. Said, you have to look at this webpage. This webpage is on what? It's on the NSF webpage. I'll read this line to you, okay? And this line says, IAD will initiate a transition to quantum resistance algorithm in the not too distant future. This is NSA. Quantum resistance is the same word as post-quantum. So what does it mean? It means in 2015, NSA decided, United States government decided, that they will change from traditional cryptography to what? To post-quantum cryptography. Okay? So US is very interesting. US, even though NSA decides to do it, but the standards in US are not by a defense department. It's made by what? Commerce department. And the commerce department is an agency called NIST, which stands for National Institute of Standards and Technology. They start to take action. Okay? They say, so they start to make standards. Okay? They say this will be dissolved and so on. And they say, we have to make two standards. Again, number one, key exchange for secure communication. Number two, digital signature for what? Authentication. Okay? Okay? Okay? At the moment, we have these families of post-quantum cryptosism. Code-based, hash-based, isogeny-based, lattice-based, multivirate. Okay? These are all based on mathematics. Okay? Then, they made a call for standard in 2016. December 2016. Okay? December 2016, we made a call for standards and we go through the procedure and they have a criteria. So, security costs, algorithms and implementation characteristics. Okay? The deadline is November 2017, which means anybody, anyone including you guys, can submit your algorithm and be evaluated. Okay? So, this is what happened there. And then, after six, seven years, okay? Last August, 2024, August, they made four algorithms into standard. One key exchange called the Kyber. Okay? Three digital signatures. The digital signatures are Dilithium, Falcon and Sphinx. Nevertheless, NIST claim, NIST made a claim, say, the first two algorithms, Kyber and Dilithium are what? Our primary algorithms. And we have two more signatures, Falcon and Sphinx Plus, as alternative. So, I'm one of the designers for Kyber. So, I'm involved in Kyber. And this is the reward we get from Kyber. And this is a key exchange algorithm. Okay? So, this is that. So, once we made the algorithm, what do we do next? We do what is called migration. Okay? So, U.S. government take it very, very seriously. Okay? They decided that we want to do it. For example, the initial, in 2022 already, the initial, what is called the NSMN 10, National Security Amendment 10 from the President. Okay? So, they decided. Okay? And then G7 did the same. Okay? And then, this is the timeline of a U.S. government. Let me repeat. So, U.S. government said, we must do this according to the steps. If you look at it very carefully, the deadline is more or less what? 2030. Five years. Five years. Five years. Five years. Five years. Five years. Five years. So, they are doing it in a hurry. Five years. Five years. Okay? So, this is a schedule of the U.S. government. Okay? So, this is a quantum readiness program. Okay? Yeah? And then, if you look at it carefully, this is the things become the pick up. For example, in the Gartner's issue 2025, Top Technology Trends, and post-quantum cryptography is in line with all the AI stuff. Okay? So, we know everybody wants to do the AI this day. So, they take it very seriously. Okay? For example, in this, they have a new organization called the NCCOE, National Center for Cybersecurity. Okay? Then, they are what? Building a new center, developed strategy for migration. Okay? If you're interested, you should Google PQC and NCCOE. Okay? They are over there. There's organization doing that. Okay? Yeah? So, clearly, from the perspective of security. Okay? Yeah? So, we must update all the systems in your cell phone, in your computer, in all the electronic devices. As long as you use public cryptography, you must what? Take it out and replace it by a new one. Okay? Yeah? Other devices. Okay? And, as we know, you know, cryptography is used everywhere. For privacy, for whatever. Okay? But, here the key concern is what? It's about efficiency and security balance. Normally, in cryptography, there's a very interesting fundamental characteristics. It's more efficient, less secure. Normally, it goes this way. Okay? You have to balance it. Okay? Now, let's talk about the main topic. I want you to talk about bitcoins. Okay? Or decentralized system. So, earlier, what I meant is centralized systems. Right? If I'm a government agency, I give an order. All of you have to change. You're going to do it. But, now, we were talking about what? Bitcoins. Okay? And, then, will Bitcoin be okay if we have a quantum computer? Okay? So, this is a question I would like answered on stage today. My answer is no. For example, one guy asked me before this question. Suppose I use Satoshi's idea. I change my address every time. Okay? Will it be okay? Right? Because, you should remember, the quantum computer can only attack you if your public key is exposed. So, if your public key is not exposed, we cannot get your public key from the address. Because, this problem, we cannot. So, what is, why then? Why is it not okay? The reason for me is very simple. Because, you have a 10-minute delay on the network. Which means, once, suppose you never expose your public key. But, once you do transaction, what do you do? You expose your public key and put the signature here. As long as I can jam you a little bit, then I can use a computer to what? To do what? To do what? To crack your what? Public key to get the private key and then I'll add more fees on it. Then people will accept my transaction, not yours. So, therefore, no. It will not be okay. Okay? Yes? This is impossible. And also, think about exchanges. You cannot change your keys all the time. Do you agree? The exchange normally has one public key there. So, we know we send it to the exchange correctly. Okay? So, no. Okay? Now, suppose we decided, okay? Even though it's a decentralized system, suppose we can agree, okay? Which signature to use? Okay? Then we realize, ohm. The NIST gave me three choices. Delirium, Falcom, Finnic+. Which one to choose? I think the community will fight. I think I heard from my friend said, Alan Beck or somebody said they will use a Finnic+. Okay? The Finnic+, the signature size is 48 kilobytes. One transaction, signature, 48 kilobytes. You think it's useful? No. Okay? So, this is a problem. So, what is the problem? It's a new post-quantum signatures. The tent either has large public key or larger signature or both of them are large. You will delay the transaction completely. You will jam the whole transaction. So, we have to do something about it. So, this is a problem. Okay? Normally, what? Normally, it's ten times. Also, there's also a problem of multi-sign. Okay? So, this is one problem. Okay? And then you say, oh, we can keep doing this. But you shouldn't realize, U.S. government not only says the U.S. government will finish by a certain time. U.S. government also said the following, okay? We'll disallow the digital signatures. Which means, in principle, in U.S., after the 2030 or 2035, all the ECDS are disallowed inside the United States. It has no legal status. It's not protected. Okay? So, this is in the eyes of U.S. law. Okay? Yes? And then, you say, oh, so we have three signatures. Okay? And NIST also realized something. Okay? This said, hmm, this three signatures may be not good enough. So, let's do some new one. So, therefore, NIST, in 2023, announced a new round to make more signatures. Okay? So, we want to make more signatures. Clearly, I said Falcons, Denetham, and Phoenix Plus, they are not good enough for Bitcoin. Maybe we can do something better. So, they have made a new call. Okay? For this new call, we have 50 submissions. Okay? I submit three of them, UV, TUV, and Supernova. Okay? And then, let's see what happens, okay? We have 50 submissions. And then, this signs with marks is after two days. So, which means many of them are broken after two days. Color, technology. Okay? It is not easy to design a secure signature. It takes years to verify, to study. Okay? So, this is what happened here, the new signatures, okay? And then, one more question. The lost ones. Okay? Suppose we want to migration, okay? Okay? We migrate. There are lost ones. Who should own it? How can we decide? Because a quantum computer can clear or crack it. I think John earlier mentioned already, right? Satoshi has so many numbers there. Who owns that? The first guy who has a quantum computer should own that? No. So, I don't know. Okay? And then, one more thing is that, I think this migration will be very messy. This is my opinion. Why? Because it's a decentralized system. I mean, I still remember, right? The Bitcoin community was fighting about the block size from, what? One megabyte to eight megabyte. They were fighting to the death. And you can imagine how much fight will be here. Okay? And then, one more thing is that people forgot to migrate. Okay? And you should realize that many, many of the Bitcoins, the public keys are already exposed. That means if you use a public key one time already, what? You're dead. Once a quantum computer comes, they will what? In my opinion, they will legally take your Bitcoins away. Because I didn't go to your home, right? I just look at your public key, what? A computer, your private key, and then what? I do a transaction, right? Is that a theft? I don't know. Legally speaking, I don't think so. Okay? Yeah? So the last thing, what I'm saying about mining. People ask me, how about mining? Will mining be affected? I don't think mining will be more or less, will it not be affected too much? Okay? In terms of the effect of quantum computer. Because quantum computer is not yet to be shown good at the correct what? Correct the hash functions. So what is the conclusion here? The conclusion here is I think the migration of a decentralized system is a daunting task. Okay? Given all the factors I give you, right? Which I will choose? How do we do that? Okay? And the last one, who will decide? Who will get that? Okay? So for me, I think it's a very daunting task. The whole community must very, very well work together and you can solve the problem. Otherwise, you will just branch, branch, branch, branch, with fork, fork, fork, fork. The whole thing will collect. This is my opinion. Okay? And then that's the end of my talk. Okay? Thank you very much. Okay, thank you. Thank you. We are honored to have Professor Maria Modat, NEC Professor of Software Science and Engineering Electrical EECS at NIT and also CEO, co-founder of Optimum. And the company helped to build a much more efficient and cost-effective blockchain ecosystem. So she is going to talk about decentralized memory for the world computer, a new paradigm for rep-free. So let's welcome Professor Madot. Thank you. Thank you very much for the kind introduction and thank you to the organizers for the invitation. It's really a thrill to be with you today. So I'm going to talk about decentralized memory for the world computer. I'm going to keep it at a high level. And really what I'd like to do today is to set together, you know, a framework which is very classical, but which we somehow maybe have lost a little bit in terms of our vision in going in Web3. So I'm going to try to keep it at a very high level, but very much welcome questions. If we don't have time, definitely after I'll be hanging around later on and happy to give you questions, happy to field questions and as always give you references and papers. Okay. So that's me. All right. Okay. So we're going to start with the simplest thing, which is what is a computer? Now you would say, well, you know, I know exactly what a computer is. That's true. But if we think of Web3 as a world computer, what does it really mean to have a computer? It was really great to hear from Professor Ding talk about Turing and of course the real explosion in understanding of computation that came after World War II. And part of that explosion is actually this very simple framework here, usually referred to as the von Neumann framework, also from the 1940s. And the idea here is, well, how do we define what a computer is? And this is an amazing framework because it's extremely simple and has had really incredible perennity. It really has stood the test of time. Okay. You look at it and you go, okay, very simple. Input and output. We have, in effect, the computation up here in orange and I have the memory. And I need a bus to communicate back and forth between my computation and my memory. All right. How does that relate to what we do in Web3? If we say that in Web3 we're doing a world computer, is that true? Well, yeah, it is. So here's, you know, a mapping. We can discuss whether you would do exactly this mapping, but it's very, I'll argue it's a very credible mapping. I have my processing unit, so my arithmetic slash logic unit. I have an Ethereum virtual machine. Let's say that it's something along the lines of an operating system. We can argue whether it's exactly an operating system or not, but let's say for the time being, it sort of is. You have a control unit. So say your smart contract execution. It's dictating instruction flow, tracking state transitions. We have, of course, some memory, which is storing both the data and instructions. And Ethereum does that. It holds a smart contract code and the data on chain. We'll talk about this on chain data later. And then we have other memory here in the blockchain ledger, which is storing transactions. That's also part of the memory in my von Neumann computer. And then, of course, we have the input and output mechanisms, right? I mean, if you compute and don't tell anything what you computed, it's not very useful. These are the transactions from users and they're the smart contract interactions, which are acting as inputs and outputs. Okay. So here's a mapping basically between the von Neumann sometimes also called a Princeton architecture and the Ethereum equivalent. Now, you know, not everything has to be a full computer. You know, it doesn't have to be going back to Turing, Turing complete. You can have certain systems that only compute certain things like Bitcoin. We've heard a lot about Bitcoin in the last two talks. You know, you're not going to use Bitcoin as, you know, a general virtual machine, but it is a machine that computes certain things and computes them well. Okay. So if we have the world computer, how are we doing in terms of taking the decades and decades of research in how you build a computer and applying them? In Web3. In Web3. Okay. So let's look at the Web3 stack. And let's think of, you know, the usual stack. Again, we can talk about whether that's exactly what you would put in or there. But we certainly have application, execution, settlement, consensus. And those basically go here into the computation part, right? This is sort of the computation part of Weinoin. And then we have network data and, of course, the hardware at the bottom. And those are basically going down here into my bus and my memory unit. So if we look at the top part of this mapping, it actually looks sort of what you would imagine that your computer is doing, your centralized computer. You have an operating system. Again, EVM, SVM. Again, we can argue whether they're really entirely operating systems, but close enough. Sometimes you have, as I mentioned, non-turing complete machines. But, you know, there definitely are machines that are central processing unit. And then you have applications that run on it. Because it's decentralized, you have to do some really hard stuff, right? The consensus is hard. We'll talk about that maybe a little bit later towards the end of the talk. But, you know, the mapping is pretty recognizable. So you have to do some things that are more difficult than you would have to do in a centralized system. But the way we're doing it looks similar. Now, let's go down here. This does not look at all like what your computer does. What do we have? We have really ad hoc solutions for reads and writes. For instance, mempools. Your computer does not have a mempool. Your computer has a random access memory. The way I like to think about it is, you know, if you have a closet, your memory is your closet. That's the stuff you use. The clothes you use. You put them in. You take them out. If they're clothes you never use or, you know, almost never look at, you put them in cold storage. The mempool is like keeping your clothes on the floor as a pile of laundry. Okay? So you put them in there. You go in, you smell it, you try to see if it's clean or dirty. You know, maybe we've done that in college. We've all at least had somebody in among our friends do it. That's not the way to build a computer. Instead of a bus like you would have on a computer, you have gossip because it has to be decentralized. And this is really slow, right? You look at something like libp2p. You know, it can take seconds to get around. This is extremely, extremely slow. You would never allow that in a computer. The other things that you have instead of a mempool is you may have a full node. Now that's, instead of your closet, a closet where you have everything that you've ever worn since you were a baby. Okay? So all your clothes are in there. It's so huge and so scary, you hate to go look in that closet. So because you're so afraid of going to look in that closet, you pay somebody, maybe your DA, to go look in your closet and tell you if your jacket is in there or not. Okay? So you don't even go and actually take the stuff out of the closet. You maybe do data availability. So the top part looks recognizable. The bottom part does not look at all like how we build a computer. You know, your computer doesn't have a mempool. Your computer doesn't keep a full node. It does have DA, but it doesn't expose it. Okay. So basically what happens? Well, things are slow. Things are expensive. You have slow transactions, low bandwidth, et cetera, et cetera. So, you know, we talk about decentralization. They are really good decentralized systems that have stood the test of time. Let's look at basically Web Zero, the electric grid. How does it work so well? Well, it works so well because it has an API, which is the socket between, let's say, this red part and this purple part. And that API is able to give me something that looks to the user like a dedicated battery. So, you know, I may have a big power plant. I may have my neighbor's solar panels. But to me, from the socket, from the API, it looks like a dedicated battery. Look at Web 2. Same thing. Now, Web 2, of course, is a little bit more complicated than the electric grid. But again, I have, whether you use Quick or some other transport protocol, you have a socket, again, an API, which is making all of Web 2 look like a dedicated point-to-point link. So I basically make this big decentralized mess look like it's just a dedicated link between two different nodes. Right? That's how things work. So what do we do here in Web 3 if I want a decentralized computer? Well, this part here is the red part like we had before. And remember, what we want to do is to build these APIs into the purple part. So what we're doing is we're taking this application execution settlement, all of this stuff here, and we're making APIs to make a completely decentralized system, which is Web 3, be able to provide the abstraction that's needed so that we can run this like a computer the way von Neumann told us in 1948. All right. How do we do that? So I really enjoyed Professor Ding's talk. I particularly enjoyed the whole part about the coding and, of course, a lot of the quantum-safe cryptography is based on coding. And coding generally means that you're taking algebraic mixtures, you know, in a particular way from different pieces of data. So you're saying the data is basically strings of bits. So I'm going to represent them and I'm going to do some mathematics on those strings of bits. Now, traditional codes have a structure. Usually you think of it as multiplying vectors by matrices. So, you know, we're going to represent the code. And it's a one-time only structure, right? So you cannot take a coded piece and recode it. And what we do here is we actually have a completely decentralized code. Okay? So that's what we've developed. Basically, it's based on randomized algorithms. We have a decentralized code. Because if you have a centralized code and you're trying to run a decentralized system, you're going to have a problem. Right? You can't do it. So the idea here is that you have a centralized code. And rather than look at the data as the original strings of bits, you say, these are numbers. And it's the same thing for me to give you the five numbers, the data that you wanted. Or for me to give you five equations from which you can recover those five numbers. We agree that it's completely equivalent. Okay? So as long as these equations are not unfortunate and, you know, degenerate, I can do this. And moreover, I'm going to have a decentralized code so I can take equations of equations and still get a valid equation. Okay? That's it. That's basically the idea. And it turns out then that using randomly network coding, which was developed in my lab, of which I'm a co-inventor, we have this complete composability. Okay? And you can keep taking equations of equations and going like this ad infinitum. So you can completely centralize. Nodes can take equations from whatever equations they have lying around, from whatever equations they get from other nodes, and keep going forever and ever. And then for reconstructing the data, all I need is to have any five equations to reconstruct the original data. Okay? Okay? Okay? Okay? Okay. I didn't... This was not part of my talk, but I was really inspired by the last two talks, which I massively enjoyed. Okay? Just a little aside. If I do this first, and then I put a quantum safe encryption, right? Say, MacLease, which we actually have done, the granddaddy of them all. If you do that, and you just encrypt quantum safe one of these, as long as the original data is, you know, has certain properties of uniformity, which pretty much anything in crypto does, otherwise, you know, we shouldn't be mining, we should be guessing, right? Actually, the whole thing is quantum safe. We just... we proved that a couple of years ago. So... And then you don't need to do just quantum safe on the key exchange, you can actually do quantum safe on the entire data. Very, very cheaply. So, that's in our roadmap, but I... Sorry, I was inspired, so I'm going a little off topic, but it's a fun thing connecting to the other stuff. Okay. So, remember the bus. What am I going to do? I'm going to have these pieces, and I'm going to distribute them now in this encoded way. And it turns out that this is optimum, hence the name of the company. It's actually optimum. You can... And it's not optimum just in an average sense. No matter how bad your system is, you could not do better. Since this is a fairly nerdy crowd, this is a good thing. Since this is a fairly nerdy crowd, I'll just mention one thing. So, maybe... Did some of you hear about the recent Dijkstra? The results about the Dijkstra algorithm being always optimum? Yeah. Yeah. Okay. Some people are nodding. So, that was actually done by one of my former PhD students at MIT, Bernard Hopler. And before he did that, he had actually shown that this was also competitively optimal. That RLNC is competitively optimal no matter how the network is set up. So, he had to use... He used similar techniques for the Dijkstra algorithm. So, just a smaller side. So, no matter what mess, you know, life has given you in this completely decentralized system, if your point is to get all of the data that you need in your gossip as quickly as possible, you can never do better than this. Even if somebody magically actually told you what was going on in the network, you would not care. You would still not do something other than this. Which is bizarre, right? It's like literally you could give me state and I wouldn't care. It wouldn't matter at all. Okay. Now, that's for the bus. But what about the memory, right? This read-write. Well, what is memory? Remember, we started out by saying, what is a computer? Now, you know, a lot of the talk is what is. The next part of the talk is, what is memory? Well, in the 1980s, IBM, which we heard about a couple of talks back, asked this very question. What is it that makes memory, a computer memory? What are the canonical properties that I need to have? And they came up with three. First, atomicity. So, let's go back to my closet there. You know, I can't have somebody putting a sweater in while somebody is pulling the sweater out. I can't be writing a number at the same time as somebody is reading it. Right? One at a time. It's atomic. Second, consistency. All right? So, if I go in my closet and I put in a dress in the closet and then somebody comes in and puts in a sweater afterwards and somebody looks at the closet afterwards, they'll never see that the sweater was there, which was put second, but not the dress. Okay? So, the order has to be maintained. There might be delays, but things don't get out of order. So, you know, think of things like sandwich attacks in MEV. Okay? And the third one is durability. So, durability basically means the stuff is there until you take it out. Right? I don't go in my closet and all of a sudden, like, my clothes are gone. In the same way that if I turn on my computer, I expect my computer memory to be there. Sometimes this is called the ACID framework. There's an I here that doesn't really matter for us. Okay? But this is the ACID framework to compare it to what we generally do in Web3, which is called the base, right? Which is, you know, best effort, eventual consistency, like, you know, I'll mess up, but then I'll fix it out later. You know, things like that. Basically what we're used to living with. And this is basically what you need to be able to do reads and writes. Now, this idea of having this ACID framework in a completely decentralized way was something that people started looking at in the 1980s. And, you know, there's the ATIA et al. algorithm, the ABD algorithm. There was a Rambo algorithm co-developed by my collaborator and predecessor in the NEC chair at MIT, Nancy Lynch. And, you know, so there were a couple of algorithms out there. But the problem was, you know, why aren't we using them? They relied on a huge amount of duplication. Lots and lots and lots of copies to check and then doing consensus over these copies. And it's too slow and it's too expensive. And that's why we don't do it. Not because we wouldn't want to have it, but it's just too slow and expensive. Now, coding is actually just the intelligent person's version of duplication. Right? Which is much nicer to say that duplication is a dumb person's version of coding. Okay? But, you know, that's the dual. So, instead of just duplicating, let's code the way I showed you before. And in that case, we can actually do all of this, but do it much, much, much faster and much more efficiently. Okay? So, now you can get speed up or block propagation with basically the bus, the optimum peer-to-peer. And you can have a decentralized read, a decentralized RAM, so a random access memory, where you can read and write in a way that's acid. So, atomic, consistent, and durable. So, we're building a testnet, a private testnet right now, and we're running some POCs. Do follow us. We'll have a major announcement coming, and we'll be talking at ETH Denver. Because this is more of a research, I'd like to just move a little bit to the appendix, because I want to talk a little bit more, I mentioned this before, about consensus, which is perfect, since we're in the, since that's exactly what we're talking about at consensus. And this idea of the difficulty of consensus in the memory versus consensus in the computation. And there's a, there is such a thing as a consensus number. Okay? Um, there's a great paper by my colleague at MIT, Nir Shavett, about this consensus number. But the main point is the following. The stuff that we're doing up top in the von Neumann architecture is generally here. It's the darndest hard stuff. Okay? It's hard. Right? Um, things like compare and swap. Okay? Just basically the, the, the toughest computes, arbitrary computes. In the memory side, we're just doing atomic read writes. That's actually the easiest thing to do from a consensus point of view. Okay? So, why are we able to do this decentralized RAM without blowing a fuse? A, we use coding. But B, we use the fact that that is actually the simplest thing from the consensus point of view. So, what happens if you do all your memory on chain using a consensus approach that was developed for this? So, you're using a machinery which is extremely hard because it was to solve much, much harder problems and is very expensive to do something which was massively easier to do. So, we may need to use the hard consensus up top in von Neumann architecture, up top where we're doing the computation. It is what it is. But we should not be using the hardest version of consensus and doing things on chain for memory and read writes which has the lowest consensus number. Right? It's like, you don't do things the hard way if you don't have to. And that's exactly what we're doing currently. Okay? And it's like, well, I need consensus. I need... So, just because it's decentralized and you need consensus, it doesn't mean that every consensus has to be the worst case consensus. Right? So, and it makes sense to keep your memory in the von Neumann architecture with, you know, the lowest number consensus. And with that, I hope to chat with you some afterwards and thank you very much. Next, let us invite our next keynote speaker, Professor Chao Jian-nong from our own university, the Hong Kong Polytechnic University. Professor Chao is the Dean of Graduate School, Otto Poon Charitable Foundation Professor in Data Science, and Chair Professor of Distributed and Mobile Computing. Professor Chao is going to talk about Web3 approach to decentralized and collaborative AI. Let's welcome Professor Chao. Good afternoon. So, I will talk about the Web3 way for implementing AI. Okay? So, my talk is the Web3 approach to decentralized AI, but collaborative AI. Okay? So, everyone knows about Web3, right? You know, next generation of the web. Mostly, actually, well from the ownership and also autonomy of the users. So, from first generation, second generation, to the third generation, the user will be able to have the control of the value, the data, okay? And then share what kind of data and then with whom. And the core feature is that we will not have any platform. So, not like today, we have, you know, the central management platform. And it will be trustless, permissionless, user autonomously control, and then everything is tokenized. So, that actually the main features. And the Web3 system mostly will be implemented by using blockchain for the trust and coordination, and also using smart contract, okay, for automation, for execution of the code. And there are many different kinds of decentralized applications and the apps, and also DeFi, MetaWars, cross-sourcing AI, etc. So, this is just a background. And then we can see over the past several years, we see the evolution of the Web3 from the digital assets, and now everyone talk about the real world assets. And then also, we have this called the D-Ping. So, now we will be able to try to put the real world assets, okay, like houses or whatever, on the blockchain for trading, right? On the other hand, we'll be able to have a decentralized approach for building the physical infrastructures, for example, your computer data center, energy network, and then maintain the infrastructure, and then incentivize the contributors. So, you can see the evolution, okay? So, many people actually still working on the fungible tokens, the non-fungible tokens. But on the other hand, there are many research on the real world assets and also the D-Ping. Today, I will, you know, focus on the D-Ping side, okay? So, what is D-Ping? D-Ping actually is a decentralized physical infrastructure network. This is a short saying. It's a kind of decentralized application, incentivize communities to contribute resources to build a physical network. So, you can see you have device owners. You can, you know, device can be anything, network, router, or storage, okay, anything. And then, you have the blockchain support, okay? Tokenize everything. And then, you have this incentive mechanisms. And then, on this decentralized infrastructure, you can run web-3 applications. Okay, that can be third party. For example, I contributed my large-angle model to run on your decentralized building physical networks. And then, this actually builds decentralized physical infrastructure. Here are some examples. Not necessarily storage, not necessarily computing devices. They can be wireless networks. They can be sensor networks. You can co-build, okay? Co-build this kind of network. So, given such a D-Ping, we can compare with, you know, the traditional centralized physical infrastructure networks. Mostly, they are run by a centralized controller. Like, for example, data center will be run by Google, you know, Amazon, and Baidu, etc. But now, with the D-Ping, we have multiple vendors. They actually get together, right? Contribute their servers, storage, etc. And then, you have blockchain to record their contributions. And then, to incentivize to the contributors whenever they have make any services to the users. So, that actually major differences. And then, there are many different kind of D-Ping projects that are mostly, they are decentralized storage systems. Okay? So, you can see a lot of Fycoin, R-Wave, BitTorrent, etc. So, there are many, many such systems. And, more and more, you can see the computing platforms. Because we have, you know, large-length models and AI applications. So, you can see many projects like BitTensor, Render, and IO.let. So, they are all actually different kind of infrastructures built by different device owners. Okay? So, I want to talk about today about our system. We call it the DCEAI. Actually, it's a kind of our research on decentralized collaborative AI system. But then, we make it a D-Ping ecosystem to support users based on blockchain and then incentivize the contributors. So, I'll tell you why. Okay? Briefly, the history. Why we come from the original research on the H-AI to the decentralized, the collaborative H-AI built on the D-Ping principles. So, everyone knows most of the AI models are trained and deployed and then used on central cloud, right? Data centers. Because cloud has a lot of rich resources. But, when we have more and more applications like, you know, Internet of Things. They are built on the data collected from thousands of sensors. Those sensors collect a lot of data. If you want to transmit all the data to the central cloud, it takes a lot of bandwidths. It also takes a lot of time to get the response from the central cloud to country of real-time IoT devices and machineries. So, in the past several years, we see the edge computing, right? You push the computing resources to some edge devices like just a server or the base stations or roadside units, etc. to run such computing functions. But when the large-danger model dominates the computation and the edge computing actually changes to H-AI. So, nowadays, you can see we have called us the edge-AI in responding to the cloud AI. Because AI models still, large-danger models still train on the cloud. But deployed, okay? Deployed on the edge devices. And that actually is a big change and a bigger trend. So, in our institute, we have actually many edge-AI applications. So, you simply train the model and then you deploy on devices. For example, for full safety, we'll be able to use AI model to test whether the wine is a fake. Okay? You can even test the same brand of wine which year from well. And the sensing technique is very, very low cost. It just uses the acoustics. And then acoustics pass through any of the liquid. The liquid absorbs the waves. We have different patterns just like our fingerprint. Okay? So, we call it acoustic print. And then you train the model and then you will be able actually to test. Also, you know, image-to-image translation. You will be able to use a mobile phone, okay? RGB camera, capture a photo, translate into actual spectrum photos which actually lead cameras, you know, like thousands of US dollars. And then now we will be able to take a photo of the food and then immediately we will tell whether the quality, et cetera, of the food. And also for monitoring of behaviors of human. For example, the Queen Nears Hospital and originally they want to monitor the patients' behavior, forward down, et cetera. They don't want to use camera. They say, no way. That actually has a big confidentiality, progress issue. But then we tell them everything is in the edge box. So, we give them the edge box. All the data, all the AI model is just in the edge box. And one box up to six cameras. Then you deploy the edge box in the patients' room. The data will be controlled by themselves. So, they feel very safe. They just, you know, deploy the edge AI things. And more importantly, you can say edge AI robots. I think this is a big paradigm shift. Before, the robots called intelligent, but they still rely on a back-end data server. Okay, to do all the analysis and then make intelligent decisions. And now we'll be able to deploy the trained AI model onto the robots. So, for example, the online pop robots. We just send the robots, go down to the pipe, and then automatically detect, you know, all the leaks or whatever. Send back the real-time information through others. Okay, so that actually is a big change. And a further step. When we have more advanced IoT application, for example, if you need large-scale coverage, not only cover a single patient room, but you cover the whole hospital, you cover the whole campus, you cover the whole Hong Kong city, right? Then the single edge box is not enough. And if you wanted to have edge-to-edge collaboration to provide the more powerful support for large energy models, then single edge box is not efficient. So we have a project actually supported by Hong Kong Research Council, and that actually is a multi-university project for developing a collaborative edge AI system. So basically you have many edge boxes. They will be able to connect it through any kind of network. Okay, wireless or internet, whatever. And importantly, all the edge nodes, they will share resources. Okay, they share the memory storage and computer resources. So I think I'm already talking about decentralized storage memory, and that can be implemented on the abstraction level. Okay, and then on top of this, we have scheduling. So you have a large-angle model, request coming, and the large-angle model will be divided into small models, and then each small model will deploy on each of the nodes. And then when we have a request coming, for example, for generating video or generating image, then we actually schedule your request to multiple nodes to implement. So this is a whole framework we call collaborative edge AI. And then we have many applications, okay, about this. For example, for the real-time monitoring, okay, video surveillance covering the whole campus. And for metaverse, okay, digital teams, if you want to cover a large area, and the collaborative edge platforms will be able to collect information from the real world, and then map to the digital world. And more recently, we actually implemented larger-lander models on the edge devices. For example, we have these robots for hospitals. Now, the hospital has more than 100 operations every day. Before, they need to have nurses to talk to each of the patients, collect information, ask questions, and then after the operation, do that again, and then generate a report. They need a lot of labor work. They don't have such resources. But by deploying larger-lander model, and then trend by their fine-tuned by their existing reports and questions, then we just have a single robot just moving around between the patients, asking questions, generating reports automatically. So that is a typical example. How can you implement larger-lander model on the edge device without connecting to the cloud, okay? You don't need the internet connection at all. And then we have done this. So we actually implement, for example, the open source, of course, NAMAS 3. Now we implement the DeepSeq models. We divide them into small models or implement their small models. And then you can have a different number of nodes for implementing different sizes of the larger larger models. And then you can see that when we actually try to design this, it's coming from a single stack holder in mind. Because all the edge boxes, the network deployment, all actually by our team. So when we actually try to deploy in a city scale or cross-city scale, it's not possible, right? Even in our campus, we deploy 30 nodes. Already takes a lot of students and also a lot of time. So hardware cost and also human cost. And then they give us actually a question. Can we do it in decentralized way? Because many people working on the edge devices, they can provide edge devices. For example, some cover central robots are coming. Hello. So, and then how can you actually deploy in the central, another one deploying in the jian sa zui. Okay. And then we will be able actually, we provide the blockchain to record all the contribution resources and then incentivize all the contributors. So that's why we actually come out, we call it decentralized CEAI. Okay. So that's the basic idea. How can we do that? And then we actually build such a system and with our own collaborative edge AI system. And then you can see we have edge clusters built from all the device owner contributing the computing power. And then we have the large language model developer and contributed their language models to be deployed on the edge devices. And then we have the users. So user can request image generation or video generation from the large and remote deployed there. But so far now we have some third party called AI gateway to receive the user requests and deploy or schedule to our decentralized platform. And also they receive the users' inference request and to verify whether the inference has been done correctly, et cetera. So this is our current infrastructure architecture. And then we have this demo actually the system I already put into use. But we want to make it real used by multiple parties and then we need to actually do more. But I just want to compare our DCI with existing systems in terms of the user device relationship and device-device relationship and whether they have let you large language support, model support or do they have any security protocols? For example, two things. If you say, oh, I have 100 GPU nodes I want to contribute. Do you believe it? How can you verify it? Like this is a very, very difficult problem. Another one says, oh, okay, I have 100 GPU nodes and you already use my nodes to run the larger model. I'll give you the result. Do you trust they really run the model? The result is really coming from the running of the model? So this is called a computation verification problem. Okay, so actually only we have this kind of protocol done and others use ad hoc solutions for doing this. So I just want to focus on the two protocols. Okay, one is the computing power verification. Verify if vendor provides computing power as claimed. Okay, now for existing projects, render, they just use a standard software benchmark to run. If you pass it, we say, oh, you have this kind of ability. And either that, and they have a minimum hardware requirement, then they do the testing. Whether you pass the requirement test. And others like BitTensor and Theta, they don't have verification. They just simply have a task. And then the device controllers try to beat and to provide the result, et cetera. So all those ad hoc solutions have some limitation. For example, they cannot actually prevent the severe attack. A vendor simply pass your test one time, but later, okay, it take away some of the resources. How can you do that? Okay. And also, the computing power pool is not stable. I contributed at the beginning. After one day, I retrieval half of this because I don't have any incentive, right, to stay there. So those actually are the problems. And another one is called computation verification. Okay. So how can I actually do it? And the current system, for example, the render, they just a reputation mechanism. They just simply based on the usage history to classify the renders to be highly trusted or just priority. And then for economy. So according to your reputation level, they actually pay different, okay, tokens, okay, for your services. And BitTensor, et cetera, they just use the redundant computation, okay, by so-called the trustworthy nodes. So they must have some trustworthy nodes that verify any other, okay, device owners and to see if your computation is correct a lot. And also, this is very new and zero knowledge proof. But this actually is used in CETA claim that in their paper, in the water paper, but it actually takes a lot of overhead, okay, to do this. So how can we handle those? So because of the time limit, I will go very quickly. So for our computing power verification, we just use what we call a pre-stacking token. So that means make your attack cost very high. So no matter who, you pay some token, okay? And then once you pass the tests or once you already verify you have resources, then you can actually trade the token into some publicly usable tokens, okay? So in that designing of this, then you also decide when you can actually exchange your claim the token to the publicly usable token and what is the exchange rate, et cetera, okay? To encourage you stay in the pool as long as possible, okay? So this is our solution for computing verification. And then for the computation verification, we use a full tolerance away. I think Maria probably are very familiar with this. If you assume you have, say, F failure modes, then you need a 2F plus 1, okay? To make sure you will be able to verify it. So our problem is that given a large amount of partition into K shots, and we have K computing edge devices for running the K shots, and then we have N validating edge devices. At the most, F can be fail, can be 40, can be 40. And then how can you design a protocol that you can validate the devices, whether their shots results will be correct or not? Okay, so this is the problem. And then actually we have two versions. One is a very naive version. You just run the verification by the 2F plus 1 validating device in a sequential manner, but that actually takes a longer time. So then we have a popular method. We divide the 2F plus 1 nodes into K groups, and then we run them in a parallel manner. In round 1, group 1 verify device 1, okay? And group 2 verify device 2. But in round 2, group 1 verify device 2, and group 2 verify device k. As long as any of the nodes pass F plus 1 valid test, then we can claim, say, my result actually is correct, okay? So when we talk about this, we talk about decentralized computing node, but how can you build the network? Now we can say we can use internet, we can use 5G, we can use Wi-Fi, but still the network building also is a big problem. So can we further develop or extend the Web3 concept to the network construction? So that actually is our current research. How can you have a deep in networking? The routers and the getaways and all actually come from different vendors, not from the central network providers, okay? And then you have research issues like how can you provide a stable network connection, right? And how can you have task well, bandwidth allocation, and how can you provide incentive to the networking component providers, and then the incentive network computation there. Now, with this actually you have another one called in-networking computation, because the network components can run some of the computations. Okay, so that can be part of the computing devices resources. You can even, you know, deploy a large sub-model into it. So the networking is not on full providing communication services, it can also provide the networking services. And actually we come up with a general architect for any kind of a deep in. And based on this hierarchical architecture, we are research on the renewable energy network. And this is a very, very hot area. Currently, the electricity actually is dominated by the grid, at least in China, called state grid. And then we have many, many renewable energies, okay, generated by the wind, by the sun. But those energy resources actually cannot be contributed, cannot be distributed. They can only use by themselves. So even they have a lot of generated, but they cannot use by themselves, it's just wasted. So what we actually want to do, we can build a decentralized renewable energy network. And then on this network, we can use optimization, scheduling to have stable output. Currently, the state grid does not want to use such renewable energy network because they say they are not stable. They call it dirty energy. But if we can decentralize that with the schedule, and then we can generate stable energy, and then we can contribute to the state grid, then we can combine this. So this is a very trendy area because many of the companies like Ma Yi, Xina, okay, because they generated energy storage devices, they have a lot of customers. So they are very much interested in doing this. So that's all, actually, for my talk. I think it's perfect. So the time is zero. Thank you very much. Thank you so much, Professor Chao, for the excellent talk. And I would like to pass the time to my colleague, Professor Daniel Law. Hello, everyone. I'm Daniel, Professor and Associate Dean of our new Faculty of Computing and Mathematical Science at PolyU. I'm also the director of Research Center for Blockchain Technology. At PolyU, together with our Research Center for Blockchain Technology, we are committed to advancing innovative research and working together with our industry partners to address challenges in Web3. Moreover, we also offer the first MSE program in Blockchain Technology in Asia to nurture the next generation of talents for Web3. So we believe that by bringing together diverse talents, we can inspire new ideas and accelerate progress in this fast evolving space. With that, we now move into our panel session. We are honored to have an outstanding group of experts for this panel discussion titled Future Unchained, Exploring the Next Wave of Web3. I cannot wait to hear our panelist pioneering research and their thoughts on the future of Web3. Without further ado, I would like to introduce our distinguished panelist, Dr. Li, Associate Professor at Nanyang Technological University and Associate Director of NTU Center in Computational Technologies for Finance. Dr. Duan Sisi, Researcher at Tsinghua University. And Dr. Patrick McConaughey, Researcher at Arbitrum Foundation. I am also very delighted to be moderating this session with my partner, Professor Eleanor. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. So Dr. Li, Dr. Duan, Dr. Magori, thank you for joining us today. So to start, let's get to know you guys a little bit better. Maybe each of you briefly introduce yourself and share a bit about your research focus in Web3, and what key questions or challenges you are currently exploring in this space. Maybe we start from Dr. Li. Thank you a lot. So it's great to be here in Hong Kong. So you know I'm from Singapore. I'm currently Associate Professor at the College of Computing and Data Science, NTU. So I do research in software engineering and software security. So trying to answer questions like how to build high quality software systems, how to build quality software systems with less effort, and also how to make sure what you build is that user expected. You're building the RET system. So I find these issues also exist in the Web3 space and blockchain space. So for example, when you write smart contract code, how to make sure the code is working correctly, and how to ensure the smart contract has good quality with automated support like testing and verification. So I'm really looking forward to collaborate more with the Web3 community to ensure a healthy ecosystem. Hi guys, good afternoon. My name is Sissy Duan. I'm currently with Tsinghua University. So my research is in the intersection of distributed system security and cryptography. So I'm particularly interested in the design and analysis of consensus protocols, including blockchain consensus. So my research fits well with the topic of the event. So in the past, I've done a lot of different works on designing more performance consensus protocol to improve the throughput, lowering the latency, and improving the scalability of the protocols. So some of my works have been used in the industry, including a protocol that is currently used for cross-border CBDC and hyperledger Iroha, hyperledger fabric. They all use some of my protocols. Recently, I've been also working with Ethereum to analyze their consensus protocols well. So right now, one particular topic I'm particularly interested in is the so-called economic security of the blockchain consensus. Yeah, that's it. Oh, great. Hi everyone. My name is Patrick McCoury. I guess I did my PhD in cryptocurrencies from 2013 to 2016. Back in 2013, as you can imagine, the industry is completely different to what it was now. You know, Ethereum didn't even exist. It was just Bitcoin in the Elkcoin worlds. I left the academic system in about 2019. I tried to do a startup during COVID. You can imagine how well that went. And then now I'm at Arbitrum. And then historically speaking, most of my research has been focused on blockchain scalability. You know, how can you take, you know, how can you perform an execution off chain, bring it back on chain in a trustless manner? Rollups solve that problem. So that's really cool. That's why I work for Rollup today. But recently, a lot of my work, and I sort of fell into it accidentally, to be honest, has been focused on governance and DAOs. Is anyone here a fan of DAOs? Like, raise your hand if you know what a DAO is. Raise your hand if it stresses you out. Well, it stresses me out anyway. Yeah, look, there's Harry down the back. He's a big fan of DAOs. But yeah, generally about a year ago, I joined Arbitrum. I discovered there's a DAO, and I volunteered as tribute to work in the DAO and support the DAO. And it really has been a volunteer of tributeness. You know, it's been really fun. More people problems than technical problems. But yeah, still very interested in technical problems. Okay, thank you so much. Now that we have heard a bit about your backgrounds, let's talk more about the exciting work happening in your institution or organization. Dr. Li is the Associate Director of NTU Center in Computational Technology for Finance. We know that this center has been a leader in the field of Web3. Could you please tell us more about the center's recent initiatives in Web3 and what are the major areas or topics the center is focusing on? Thank you. Okay, so maybe a bit more background about our center. So our center is called the NTU Center of Computational Technology for Finance. Just a fancier name for blockchain. So our center is a Penn University initiative across many different schools and colleges to try to foster more collaborations among us to really push research, education and community development in blockchain space. So for education, right? I myself, I'm in charge of, you know, a master program on blockchain. It was very similar to what is offered at PolyU. So it's also, you know, Singapore is, I think it's in a very similar situation as Hong Kong. So the government is trying to push it to become a hub for FinTech and blockchain technology. But, you know, in Singapore there's, you know, high demand for technical person who knows how to build stuff, who know how to develop technology. But we don't really have enough supply. We don't really have a lot of, you know, technical students trained. So that's why there's a huge demand for this kind of talent development program. So we've been running this master program for more than two years. And this year we're trying to expand more in terms of intake. So starting from this year, we're going to have two intakes, one in August and another in November. We're actually actively, you know, accepting applications right now. So if, you know, you know anyone who is interested in this, please ask them to apply. So apart from the master program, we also started a new PhD program. So for blockchain research. So it's a very unique program because it's a interdisciplinary research program, meaning that every student will be supervised by at least two advisors from, you know, very different, background, for example, a computer science professor, another, you know, maybe from the business school. So I think this is important because the blockchain research itself is, you know, by its nature interdisciplinary. So you really need to know knowledge from, you know, many different areas and discipline in order to have, you know, innovations. Okay, so that's probably the education pillar. And for the research, so since, you know, we have faculties from different colleges, including computer science, businesses, social science, even communication schools. So we do very diverse research. But, you know, apart from academic research, we also pay a lot of attention in industrial collaborations. So just very recently, we have established a research lab with ZOOG Labs. So we're trying to do, you know, more developments in decentralized AI, just as Professor Cao introduced. For example, you know, to look at the scalability issues and also data privacy issues. And also maybe, you know, how to integrate AI agents into the blockchain system to enhance its capability. And finally, for community development. So we try to, you know, build a healthier blockchain ecosystem in Singapore. So at NTU, we have, you know, many student clubs. One of them is called Blockchain at NTU. So these are mainly undergrad students. So we organize activities such as hackathons and, you know, business pitches, try to help them to develop business ideas. And we also host an annual event on blockchain. So this year we'll upgrade it to a Nanyang blockchain conference. So last year, you know, we're fortunate to have invited many experts over the world to come to NTU and talk about blockchain technologies. And this year we're going to, you know, try to expand it further. But this event is mainly focusing on academic research. And this year we're also going to have a call for papers. Hopefully also to invite more students to come and, you know, interact. Thank you, Dr. Li. Now I have a question for Dr. McCurry. Dr. McCurry, as you have just mentioned, you spent your career in blockchain, but splitting your time between academic research and in the industry. Now at Arbitrum Foundation, which is one of the leading organizations for Ethereum's layer 2 scaling, could you share what projects your teams are focusing on recently? And are there any initiative or upcoming developments that you can share with us that you are excited about? Yeah. So maybe before I get into that, like, why does Arbitrum interest? And from a research perspective, you know, because obviously the founders are from Princeton, you know, Stephen, Ed, and Harry, they are a group together. So what research was it that made Arbitrum interest in the beginning with? And it's really the do of verifiable computing, which is like a very old research area for 30 years. And the premise is really simple. I give a computation to the cloud. The cloud does some work, returns me the result. And I want confidence that that computation was done correctly. But I don't want to repeat the work of the cloud. You know, I'm a mobile phone-like client. I don't have all those resources. And really that's what roll-ups are, you know, something like Arbitrum. It's just verifiable computing for smart contracts. We have a smart contract. It wants to know at all the transactions were processed. You have the sequencer, the validators, the operators. They perform the large execution. Then they provide a little snapshot that says, smart contract, bridge. This is now the new state of the system. And then the question is, how does the smart contract be convinced that this is indeed correct? Either you rely on fraud proofs. That's obviously what Arbitrum uses. Or you can rely on validity proofs, which are zero knowledge proofs in mathematics. So fundamentally, you know, Arbitrum came from a research environment. It looked at verifiable computing and thought, oh, that could work in the blockchain context. They deployed it. It worked. Everyone's like, woo, that's exciting. And now we have roll-ups as well. And obviously in industry, you know, alongside academic research, people at the EF and other groups are also working on a similar line of work. And it's sort of like the mixed mass of both. For example, a sequencer wasn't originally an Arbitrum. That came from industry. Today, there's various topics that Arbitrum's working on that is, I guess, relevant for research. Maybe the first one is to do with, you know, different virtual machines. So one thing that Arbitrum released this year and got integrated on the mainnet was Stylus, which is a Rust environment that sits side by side with the EVM. So you can write a smart, you know, a Rust smart contract. It can interact as a first class citizen with the EVM. Now the question is with Rust and Wasm, you know, what can you build with that? Like, you know, what this huge execution environment or at least cheaper execution environment, what can you build? And what we're seeing there is people are using Stylus for zero knowledge proofs. It makes Verify and zero knowledge proofs very, very cheap. And so we're seeing people build dark pools with something from TradFi in the real world, like now in blockchain. So that's cool from, you know, we implement the new VM. People are now building cryptographic systems on top of that because it's cost effective to do. Another one is the fraud proof system itself. So obviously the fraud proof system was invented in 2017. You know, that one probably wasn't very practical. I think just two weeks ago, mainnet integrated bold, our next generation of fraud proofs. And if you're interested in how fraud proofs work, I would go look at that research. It's much better than the previous version. Probably the biggest difference is in the previous version, it was gate kept. So I mean, with a fraud proof system, ideally, anyone in this room could come along, look at Arbitrum, find an issue, submit a fraud proof, save the day, protect all the assets in the system. The previous fraud proof system was fixed, as in like only 10 people could participate because of delay attacks. The new fraud proof system is permissionless. Anyone can come along, submit a fraud proof. Of course, you do need to have a lot of stake. Yeah, I think there's like 3000 stake or 3000 EVE to participate, but at least it's permissionless. So there's been more evolution on how to build a better fraud proof system. We are looking at zero knowledge proofs and validity proofs. You know, could we use validity proofs to simplify our fraud proof system? That would be great because the fraud proof system is still, that's to say, it's a lot of transactions back and forth. Maybe the third one that comes to the top of mind is governance. That's what I've been spending most of my day working on. You have a DAO, they create an organizational chart via popular vote. You know, they vote in different orgs, they do different parts of their functions. There's lots of e-voting problems around that. One thing I was looking at this week was Lobbify. They're a company, a venture funded startup, to enable vote buying as a service in a trustless way. Where you can sell your votes to the has bidder. That's in DAOs and e-voting. If you sell your vote, that's considered an attack on the system. They are now a venture funded startup to do this as a service. Anything happens in cryptos. So yeah, there's lots of cool research going on and it happens in real time. So I'm always keeping up. Cool. Thank you so much. So having heard about your institutional work, let's talk more about your research. How is your research solving the real world problems and saving the future of Web3? Dr. Duan, first of all, a congratulation on receiving the prestigious Intel Award from Ant Group last year. I know that your work has been used in many systems across the industry. For example, the Enbridge project, a cross-border CBDC payment project, is using your Dashing consensus protocol. Could you please tell us more about the Enbridge project and how your protocol is playing a role in this project? Yeah, happy to. Thank you. So the Enbridge project is led by Bank of International Settlements. It's cross-border projects for CBDC cross-border payments. So BIS, Bank of Settlements, is leading three cross-border payment projects. Two of them are centered in Singapore and one of them is centered in Hong Kong. So the one centered in Hong Kong is called Enbridge. It's currently run by a few sites, including Hong Kong, mainland China, Thailand, and United Arab Emirates. But there are other countries, like banks, central banks, all over the countries, they are serving as observing members for the projects, including pretty much every country we are aware of, like United States, France, Australia, etc. So when we started to work with the team on this project, they were in the lab phase. So they were using Tendermint in their first version and Hustuff for their second version. So they were in the pilot stage later on, and then they were deploying that among the few sites. So in the bank system, central bank system is very special. They are not running the public network. So they are like, each two banks are connected with private channel. So when they deployed that and then evaluated the performance, one thing we were told of is that there were a lot of moments where no blocks were. No blocks can be committed on chain for like 20 minutes, for example. So, but after we started to look at the logs, the system logs, what we realized is that sometimes it's because of the network connection. A lot of times when we start thinking about designing a consensus protocol, we are thinking about the situation where every pair of correct nodes can be connected well with each other. So there are probably not so much time where the connection is bad. But for the kind of the special bank system, it's sometimes quite common, actually, when the network connection is not that great. So that brings us to about the problem of some problems can be solved by engineering team, but some of them cannot. So one thing we brought to ourselves is that, say, when we design the protocol, we always think about an ideal situation. But when the network is not that ideal, what can we do? So what we did for the protocol is we kind of somehow play with the number of votes we need to collect for the consensus protocol. So usually when we say we have N equals 3F plus 1 nodes, we need to collect 2F plus 1 votes. And then what we did is instead of waiting for all the votes where they may come after 20 minutes, why don't we just reduce that number? So in the end, we design a protocol that can still work with F plus 1 votes. So that's about half than before. But still, we can ensure the correctness of the system. We still do the formal proof and everything is correct. So that's what we did with the team. And right now, they are upgrading that to the version 2. And then it's currently in the MVP stage and everything seems to be going on well. Yeah, that's kind of the history of the protocol. Yeah. Thank you, Dr. Don. That's really exciting to hear about the project Enbridge. And we are excited to see how Enbridge and your Dazzling Protocol is shaping the future of cross-border payments. As this blockchain system are being integrated into our financial systems, security actually becomes a critical issue. And especially for applications running on blockchain, like decentralized application. So my next question will be directed to Dr. Li on DApps security. Well, DApps have grown significantly in recent years. And your current research focuses on ensuring the security and reliability of DApps running on blockchains. Could you share some of the key security challenges in this space? And how has your team been addressing these issues? Okay. Thanks a lot. Yeah, indeed. I've been working on a lot of security for smart contracts and also decentralized application in general. So in my opinion, I think a key challenge for us right now is we are being overwhelmed, right? Because the kind of security issue always changes and there's always new problems discovered. And then, you know, for example, earlier stage there's re-entrancy and, you know, people get started to be concerned about this and then develop a lot of tools and techniques to try to identify re-entrances in your code. Okay. This right now, you know, with, you know, Solidity become more mature with a lot of security libraries. Then this may no longer be a huge concern right now. You know, but there's new issues coming such as, you know, flash loans, things like that. So new type of attacks always emerge with, you know, evolving landscape. So I think for us, instead of chasing after one issue after another, right? Identifying an issue and then somehow define it using a pattern or some signature and then try to search for those issues. So for us, I think a much better solution would be, you know, find a way to define what is the right behaviour of a contract. Instead of defining what is wrong, right? But we realise that it's much harder to define what contract is the right contract. Because in order to specify the right behaviours, you will probably need to, you know, have very precise, good semantics for the programming language. And then also need to have a very good understanding about the intention of the developer. Because only the developer are the person who has the final say about what is the actual expected behaviour of the contract. What is so called the right behaviour. So to solve that issue or maybe to push towards that direction a little bit. So what we've been working on for the past few years is to try to automate this process of defining what is the right behaviour. Using, you know, some specification language. So of course, to fully automate that is very, very difficult. Because we're not the developers themselves. But we try to develop tools that, you know, try to have a partial specification. That will serve as, you know, a good starting point for maybe to developers to look at and then continue working on that to complete the whole specification. With the specification of the right behaviour, it will solve a lot of issues. You no longer need to be stressed out when the new, you know, security vulnerabilities are discovered. You can use, you know, a lot of pre-established testing and verification technique to really, you know, logically prove that the contract is doing the right thing. Right. That's one part of our research. Another thing that we are interested in is, you know, from the perspective of users, right? Users, they are not expected to, you know, read the smart contract code. They are just, they just want to, you know, take a protocol and start to use it. But how can they, you know, develop a trust towards your application, right? If they just look at, for example, the web paper or documentation, how do we make sure that what is written in the web paper or documentation is consistent with the actual implementation of the contract? Right. So to answer that question, we have developed some, some techniques to do this checks, consistency checks to make sure, okay, whatever is specified in the web paper is indeed implemented in contract. If there's any differences, these should be exposed and users should be let know, right? For example, recently we have published a paper studying the governance mechanisms in a lot of DeFi particles to really, you know, go to the code level and check the claims made in the paper are indeed implemented as is. Another thing that we do is also to make it accessible for average users to start writing their contract. For example, if you don't, maybe you don't have a lot of knowledge in Solidity, but you do want to develop your contract. So we're leveraging, for example, generative AIs to translate some legal contract written in natural language into smart contract code. So I think those technologies will really make a lot of traditional techniques we already developed more accessible to a larger audience. Thank you Dr. Li for highlighting the challenges in dev security and sharing your promising solutions. Dr. Duan, since you have also made a significant contribution to the security and resilience of blockchain, how do you see the current state of blockchain security evolving and what are the most critical security challenges for blockchain ecosystem nowadays? Yeah, so there are still many challenges. Yeah, so people are still working very hard for the better performance of the protocols. But one particular thing I'm particularly interested in in recent years, one year or two, is what I mentioned, so-called economic security of the system. So by that, what we mean is that what if the assumption of the consensus protocol is violated, right? So we always know that for proof of work to be secure, the adversary cannot control more than 50% of the money power. For proof of stake to work, the adversary cannot control more than one third of the stake. But what if that assumption is violated, then everything is broken, right? So that's kind of what we call economic security. There are two aspects for this topic. One is how to make validators motivated, right? So that's related to the stability of the incentive mechanism. Another topic is even if the assumption is broken, we still want something good in the end. For systems like Ethereum and Bitcoin, it's hard to imagine that the assumption will be violated. But for other chains, it might be, right? So they are much smaller. Yeah, in that sense, how can we better protect the system? So to look at the problem, there are different angles, right? So we can look at from the design of the protocol perspective. We can also look at from economic security of the problem. And we can look at that from both directions as well. So for example, one thing from the technical way is that we can somehow rely on some way of what we call social consensus, right? So everyone is watching how the chain is going. If something is going on wrong, there are hard forks, how can we merge to one fork? That's one way. But we can also design the social consensus technically, right? We can implement that in the protocol. When we design the protocol, we can provide some recovery mechanism or some fallback mechanism. So even if the assumption is violated, we still know what's going on wrong. And then we probably can do the recovery process for that. So that's one thing that's particularly interesting to me. Another thing is from the incentive perspective. So we are looking at the intersection of game theory and distributed system protocols and cryptography as well. So there might be very elegant solution for that. And we are still looking at that. So with that said, how can a consensus broke still be secure under rational players, right? So rational but Byzantine players. So these are some topics I'm particularly interested in. And I've seen some research going on all over the world. I've seen some groups that have put some preprints about these type of topics. Yeah. So that's kind of the main thing I'm working on. Yeah. Thank you. We have covered consensus mechanism, DApp security. So let's talk about what's next for Web3 development. As Web3 continues to grow, so the demand for faster and more cost effective solutions is also there. The growth has driven the rise of layer 2 technologies, which have helped scale blockchain networks by reducing conjections and lowering transaction costs. So this question is for Dr. Macquarie. So we know that you are a very strong supporter of layer 2 solutions on Ethereum. So how do you see layer 2 evolving to support this next wave of Web3 tech development? And are there any emerging trend in layer 2 you are excited about? Yeah, that's a great question. So I'm going to ask the question to the audience because this will validate my point before I get to my point. So just hopefully you're all paying attention. Don't know if you're all awake yet. But raise your hand if you've ever lost money on an exchange because it got hacked. They disappeared. They ran away of your funds. Has anyone here lost money? There we go. Maybe FTX. Hopefully you got refunded from FTX anyway. Probably the one good case where people got refunded. I mean, over the years, like I remember in 2016, 2017, 2018, every two weeks an exchange got hacked. It was ridiculous. I think it was like the Canadian exchange. The founder was the only guy with the keys. He went to India, died of Crohn's disease and disappeared. You know, all these stupid stories, which are absolutely ridiculous. So why is that relevant, the roll-ups and L2s? Because when you have an exchange or just a cryptocurrency service building Web2 technology, that company has to protect and take custody of billions of dollars. You know, they have to have an entire security engineering team to protect the systems from obviously the adversary. And if they make one mistake or one slip up, well, you know, the hacker gets in, they steal the funds, they run away and, you know, we have to go chase after them. Maybe one famous case was BitPhenix. They lost 120,000 Bitcoin. And the hacker in the end was this girl who was like his online ropper. I forget her name now. He just got put in jail, I think, or whatever. It was ridiculous. So what's nice about L2s and roll-ups is that they look very similar to that Web2 technology stack. You know, you have the sequencer, you have the smart contract platform where you can deploy DeFi. So you can recreate the experience of a centralized exchange. But now you can rely on the tech stack to protect the funds. You know, I deploy the bridge. I deploy the off-chain infrastructure. You, the user, you come along, you deposit your funds in the bridge. They pop up on the service. You play with DeFi. You know, you do your lending, leverage. Maybe you buy your meme coins. I don't know what you guys get up to. But what's nice is that it's the tech stack and not humans that are protecting your funds. You know, if there's a bug of the operator's malicious for whatever reason, you can come along, submit a fraud proof, protect the system, save the day. If the operator tries to go offline and censor your transaction, you can submit a transaction on Ethereum and it will eventually get processed and then you can withdraw your funds from the system as well. So if the entire system goes offline, you can still get your funds out. So to me, that's what I'm really excited by for RollOps because they solve a real problem of building off-chain systems and protecting users' funds. But one thing that worries me, and this is sort of what I would like to see more, it's not even a research, but it is a research problem, but it's not a technical research problem. Protecting the custody, you know, protecting your funds and interacting with crypto is still really hard. I don't know if anyone here has ever used Gnosis Safe, you know, multi-sig platform. It is the worst user experience ever. And that's how people protect billions of dollars or interact with DeFi if they're using multi-sigs. So really a big problem now is not necessarily the infrastructure, it's the user experience and just making it easier to use Web3. Because if that isn't solved, then even if we have this cool infrastructure, people are still going to use the option, they'll still use the custodial option and still trust their parties with their funds. And then you haven't really solved the problem at all. So to me, that's really one of the big outstanding problems now. Thank you so much. So I actually, I would like to thank all the panelists for sharing such a variable insights. Today we have already covered lots of topics, including consensus protocol, security, scalability, and the future of Web3. Before we close, let's open the floor for a couple of audience questions. Please raise your hand if you have any questions for our panelists. All right. And I have a question for Dr. Li. So thanks for your sharing, for your security solutions. My question is that what is, in your opinion, what is the trend in Web3 security or Dapp? And does it bring any future opportunities to some PhD things? Okay. Thank you for that question. So one trend that I'm observing is the security issues that concerning us is moving maybe from the low level programming bugs into the higher level, you know, logical issues. For example, as Dr. Duan mentioned, these economic security issues. These are, you know, contracts which are implemented correctly without a bug. But, you know, depending on, you know, how people are using it. So, I mean, there's no code issues, but there's still, you know, it can still be considered a secure issue. So that, of course, creates new opportunity for us. So one probable direction is like what I mentioned. Can we have a better way to define, right, what is the business logic that you want? What is the correct behavior that you expect? And once we have that model, and then we can, you know, try to apply maybe some formal methods to, you know, simulate different scenarios and see if there's any issue with that. Or even mathematically proven, you know, there's no economic security issues. So these, of course, I think are more challenging than those, you know, coding bugs. Because with language getting better and better, it's harder to make mistakes. But still, how to ensure the logic is right is, you know, always needs some, you know, human, you know, ingenuity in those, solving those issues. Thank you. Thank you. Thank you. Yeah. Hello, Dr. Duan. I have a question. According to my understanding, our assets on the chain is secured by both digital signature and consensus protocol, theoretically. And we've already known the consequence of broken crypto signature. But I have some question about the consequence when we have a broken consensus. We've already heard something like double spending, et cetera. But I want to know what exactly the percentage of assets will be lost when the safety of consensus is broke. Thank you. Yeah, good question. If safety is violated, then everything is broken, I will say. Yeah, of course, there are two, if there are two hard forks, then every double spinning is very easy. But it's, there's some, maybe some theoretical bound on what will happen if the adversary controls what percentage of the stake, for example, or the percentage of the validators the adversary can control. Say, the R assumption is F, right? One third, usually one third of the stake or one third of the validators. If we say, if the adversary controls one more validator, then the possible consequences might be very little. But if the adversary controls more than two third of the stake, then everything will be totally broken, I would say that. Yeah, that's kind of what safety, what consequence will happen if the safety is violated. If the liveness is violated, I don't think rational players will do something bad because they won't receive any rewards at all. But what if the adversary just wants to do something bad, right? So usually the evil person just wants to do something bad. And that actually happened once in the real world. So there were something called control over for the pos, delegated proof of stake. For some systems, there is a large amount of takeover. So one party takes over a huge amount of stake. And then all these players are discouraged. They leave the system. So with that said, I think if liveness is violated, there is also some consequences. But it's hard to quantify them anyway. Yeah. Can I add one thing to that as well? Because it's also relevant for Bitcoin and Ethereum. It also comes down to finality. So if you think of Bitcoin, for example, it's proof of work. It's objective. You can check the longest proof of work and you can see what the chain is. Breaking safety there would allow you to roll back blocks and put in a new fork. But it depends on the adversary's power on how far they could roll back. So can they roll back two years, one year or a week? So it really depends on how much more power they have over others. And then in Ethereum, finality is subjective. So if you're online at the time, it's finalized every time. It's finalized every 12 minutes. So if safety is broken after that, then people who were online will ignore the... You can't reverse to go back in the past, but you can really screw up the future. So I think finality is also important for that. You know, it's... The break depends on when do people say, oh, this is what I consider final, and they'll never, you know, revert or go backwards on. Then anything forward is really vulnerable. Yeah, exactly. Yeah, exactly. Yeah, exactly. Hello, good afternoon. Thank you very much for the symposium. It was very engaging, truly. My question is directed to Dr. Duan. However, I would love to hear the opinion from all of you. Since you're working on Enbridge and projects to do with CBDCs, how fast or how likely are they to be implemented worldwide? And what are the consequences for privacy and decentralization? Yeah, very good question. So for that project, currently we are using the conventional BFT protocols. We are not using any proof of stake type of model because we are using blockchain as a ledger to store all the cross border payments. So for that, the development process is not that large. It's very pretty easy to deploy that to a new site. Yeah, that's a short answer to the question. Yeah, and then for privacy issues, it's indeed a very big issue. So there are lots of governmental policies and policies regarding privacy and security are different for different countries. So that's actually one of the main challenges we are currently working on. So we somehow want the government to oversee all the relevant transactions that are regarding to each country. But a lot of the parties are not willing to reveal their transactions to other countries that are not relevant. Right. So, for example, if there's a transaction between China and Thailand, Chinese government and Thailand governments, they want to see the transactions. But they probably don't want other countries to see that transactions. So that's the main privacy concern. There are some solutions and some of them can be borrowed from the cryptocurrencies. But I don't think they are perfect yet. We are still working on that. Yeah. Thank you. Thank you for impressing and sharing. I have a question for Dr. McCorry. And I would like to hear the opinion from Dr. Duan and Dr. Lee. So the question is about censorship resistance. So how do L2 and L1 guarantee censorship resistance in general? So, for example, in Arbitrum, so you have a centralized sequencer. So how could Arbitrum guarantee censorship resistance? And for L1, so now we can see Ethereum ProStake chain. So two builders generate more than 90% blocks. So it's indeed a potential issue. So I would like to hear your opinion. No, that's great. Yeah. So that's a very deep question. Let's start with it. So basically Arbitrum inherits a censorship resistance from Ethereum. So that's probably worth tackling first. And just to reiterate what you said, in Ethereum today, there's two builders that build like 90% of all blocks or something along those magnitude. And a builder is someone who works with the staker. So, you know, they create the content of the block, they pass it to the staker, they then obviously just make a block with that content, with that list of transactions. So in Ethereum, what you're really relying upon are the solo stakers, the people who are not participating in MEV activities. They're just listening to the P2P network. They're picking the transactions. They're following the rules. They're acting as honest parties. And they include transactions as they can. And a good example of that was the OFAC sanctions against Tornado Cash, where if you, you know, would you be breaking sanctions if you were to include a transaction that interacted with Tornado Cash on Ethereum? And a lot of the solo stakers were the ones who upheld censorship resistance, and they would include your transaction eventually. So typically on Ethereum, maybe on a normal case, you would get a transaction within three blocks. For OFAC transactions, it would take one in a hundred blocks. So maybe like five or ten minutes, but you would eventually get included. So censorship resistance there is you will eventually be included, but it may take a little bit of time. And obviously now I think the courts have said that the OFAC sanctions were removed because it was, you know, disputed in court. So obviously the solo stakers were probably doing the right thing. So Ethereum, our rollups in particular, they assume that an honest user can always send a transaction to Ethereum. You can always get your transaction included in Ethereum, then you can eventually get it executed on the L2. So that's the censorship assumption. So assuming that, you know, you know, the fact that they could defeat OFAC, which is pretty crazy, by the way, is a very good real world example where, oh, that is a reliable assumption to make. So in Arbitrum, it relies on a mechanism called forced inclusion, where if I send my transaction to the bridge, it will sit there and be pending for 24 hours. Then after 24 hours, if Arbitrum wants to make any progress whatsoever, it must include my transaction and it must order it for execution. So the entire system will halt if my transaction doesn't get ordered after 24 hours. So that's how you get, you know, censorship resistance. But when you get into detail in the weeds, there's also like little edge cases there, but generally it will forcibly be ordered for execution. So it enforces the ordering of that. And that's how it's achieved. Hello. This question is for everyone on the stage. So I have one kind of more practical near future question and one that's maybe slightly farther ranged. So right currently there are multiple chains and so on systems where you can carry out financial transactions. How do we ensure that the transactions and the asset records and so on can be safely kind of transferred between different chain segments? That's a kind of more near-term question. And the other question is about if we will talk about kind of future, if we expect that the AI agents going forward might be on behalf of people and then if we want to, nowadays say the dollar nodes, they are very scalable in a way, right? So like billions of people, they can carry out transactions at the same time. So how do you see that going forward to support not just billions of people, but each might have say hundreds of AI agents working on behalf of them? How do you ensure the scalability of the volumes of transactions? I'll give a quick answer. I'm sure others will want to jump in. I would just say that AI has always been around for blockchains. If you consider searchers and MEV bots, you know, they're, I don't want to say they're AI because obviously they're not using like the sophisticated setup, but they are bots. You know, they're valuing the blockchain and sending transactions in real time. And bots have always existed on blockchains and those bots are geared towards profitability. And as we've seen is that they do spam the blockchain. It's really a trade off of, you know, they want to make profit X. They're willing to spend up to X to make that profit. And so your transaction fees have to take that into account because if transaction fees are too low, they spam the hell out of the network. AI for users. I think there's a lot of hackathon projects that ETH global hackathons and it's sort of like JAT GDP or, you know, any of those AIs where you type in to say, help me, you know, take my ETH, lock it up in Aave and obviously earn me some yield. And then the AI will go do that. I'm really excited for stuff like that. I think if it takes off, interacting with crypto would be so much easier. That's not my short answer anyway, but maybe. Maybe. Well, I think that what the scalability one, so the problem is a bit like when you create highways, you know, in a city, on an actual highway, an actual highway, but then it just gets recongested again. Because as you increase, you know, the liens or the throughput or the capacity, if the fees are too low, then you get bots who just spam the blockchain. So really scalability needs to take into account, like the whole point of transaction fees is to prevent denial of service, an anti-spam mechanism. So there's always going to be a trade-off between the fee that you set and your ability to defend against bots and spam. So that's regardless of the scalability solution. It really comes down to like, in my opinion, fees. You know, fees need to be a deterrent to spam, but obviously cheap enough for users. Okay. Sorry. Maybe we have more discussion offline. Okay. Before we conclude, I would love for each of you to share a parting thought with our audience. Maybe we can start with Dr. Li. Okay. Maybe, you know, something to say to the, you know, PhD student who really want to do research in blockchain and Web3. I think my word is interdisciplinary. So I think you really, many of you come from computer science background, but you really need to know a lot from other areas. Yeah. Thank you. I actually wanted to say the same thing. We have been looking at a lot of interdisciplinary research in this area. I think that's fascinating. Yeah. For all the students, if you are interested, just contact me afterwards. Yeah. That's it. The only thing I would say is like, when I got started in cryptocurrencies, I learned a lot of my knowledge from the IRC channels, because that was obviously Bitcoin back in the days. And nowadays there's so much cool research being done in public on the forums, even on Twitter sometimes, God forbid, you know, CT is a special place. But I would never, I would try not to discount the research that's done outside of academia. Because a lot of that research has generally helped academia find a good path towards deploying real research. Like rule ups, partly academic, partly industry. I would not discount the work they're doing out there, even though they don't write fancy papers. They actually have cool, you know, intriguing thoughts. Thank you so much for those important advice. A huge thank to our panelists, Dr. Lee, Dr. Don, Dr. McCarney for sharing your expertise. And perspective on the future of Web3. Let's give them a big round of applause for their variable insights.