 MATTHEW BAUERFORD- Hello, everyone. My name is Matt. I lead BD for Redstone, which is an Oracle provider. This presentation is going to give you a little bit of an overview of the overlap between oracles and AI, and specifically what we are looking for and working on right now in terms of building a digital economy, a gentic economy that's yet to come. But first and foremost, when it comes to two words about us, we are actually the fastest growing oracle on the market right now, both series A, 40 people on board, mostly developers. And we work with some of the biggest protocols in DeFi currently, such as Morpho, Pendle, Compound, Venus, Spark, and many, many others, so some of the blue chips. So the traction on the oracle part has been great. There's nothing to complain about on that front. But when it comes to how we have used to grow is we have chased the right narratives and attacked specific segments of the market and be just the first oracle to do it. That has been LSTs, LRTs, Bitcoin LSTs, yield bearing stables, and a couple other different asset classes. We have specifically focused on the lending use case and yield bearing collateral for lending markets. But now we need to take the next steps or two steps. And we believe that the AI sector is definitely something super interesting. And we believe that there's going to be more and more overlap between that and oracles. And this is basically the goal and the reason behind this presentation. So AI agents, as you guys know, this is not a crypto-specific phenomenon. Everybody is excited about the silicon-based intelligence that we are building. Everybody in the traditional tech, everybody in startups, everybody in even some of the most analog traditional industries as well. However, Web3 and AI is this specific precious combination because the permissionless machine-readable nature of the systems that we are building here is just perfect for agents to proliferate. Because this is the type of a system where if you want to interact with the edges of it, you actually don't need to get out of the system. And this is not how traditional systems used to work. You would have to have human interaction before you could do some interaction with the outer parts of that system. So this is also kind of a virtual cycle between AI and Web3 in a sense that yes, Web3 is the perfect environment for AI, but also the other way around. AI is probably the perfect complementary for Web3 in terms of making sure the UX is on point, in terms of making sure the on-chain data is human readable as well, in terms of making sure that we as humans understand the risk profiles of the protocols that we interact with, the edge cases of the code that's being deployed on production, and so on. So basically this relationship goes both ways. When it comes to the levels of the stack that we see right now for agents being deployed on-chain, there is a couple of very, very important primitives that are being developed right now. First and foremost, you have frameworks like ELISA OS, so the things that actually let you build agents that can later be used on-chain. Then you have platforms like virtuals, so specifically tools that allow you to deploy agents and manage them later on. Then you have modules, so the things that enable agents to have some specific capabilities. You have analytics tools like Kaito or Kuki3 that basically give you insights about specific agents, but also a 10,000 view on the whole market, kind of an umbrella on the whole market. How it looks like and how agents communicate and collaborate with each other. And then you obviously have memes. You know what memes are mostly for fun, but also they are the sort of cultural vehicles of the vibes that we are building here, the culture that we are building here. The smallest unit of replication for the culture, as Richard Dawkins would say. Now, probably evolution is not the best word to describe the path that we have gone through so far, because traditionally evolution is understood as this very, very gradual, slow-moving process that takes a thousand iterations to get one change done. Whereas this, I would say, is rather a revolution. And also traditionally, evolutionary process does not have insights into where exactly it's going. It doesn't have a grand goal. It just sees for the next one step. Whereas here, we exactly know where we are going. We want to have general intelligence based on silicon. And that is something that we have seen from the very beginning of computers, basically. So if you really wanted to go back in time, you probably should get back to the 1940s, the first papers by Alan Turing. 1947, the first lecture in London, 1948, the first paper never published. Basically describing that we need to be building machines that are able to learn on their own experiences. So something that used to be called a Von Neumann machine back then is now called a computer. And here we are in 2025. We are post the Marc Andreessen goat moment. And since then, we've seen a proliferation of different agents being deployed on chain for a variety of different use cases. You see agents that are being basically internet personas like AI XPT. You see agents that are being used for trading, for making decisions, for analyzing risks of protocols, and doing many, many different other things. You also have platforms like virtuals for deploying them. So the infrastructure is being built. The use cases are just being discovered. And everything is kind of moving at an exponential pace right now. And that exponential nature, I believe, does have quite a lot of resemblance to how DeFi used to work at the virtual. So why DeFi was and still is so transformatory is because of the composability aspect. Because of the fact that you can trustlessly build on top of the things that have already been built in that space. And it's also a system where you can just interact with every aspect of it without going out of the system. So that's where finance thrives, as we all know. And that's where AI also thrives. So quite a lot of parallels. But in general, the market is growing at an exponential pace right now. Why Redstone, as an Oracle company, is interested in AI specifically? So what you need to know about agents is that the complexity is actually not on the model side of things or on the deployment side of things. Because we've got great tooling for both when it comes to models. You can have open source models, closed source models. One is better than the other. You can have platforms for deploying agents. So that's the easy part. But if you want to build something like AI XPT, so basically a crypto influencer that needs to stay relevant at all times, probably one of the crucial parts or one of the trickiest parts is just to get the data pipelines right. So the data pipelines means all the data that goes into the agent on which the agent makes decisions. So basically how agent tells truth for bullshit, right? If you have a situation with garbage in, garbage out, which is usually the case if the agents are using just general purpose data scraped off the internet, they are never going to become the best in their category. And if you really want to have an agent that is the best in what they do, you need to have very streamlined, very efficient data pipelines. So the quality of the data that goes into the agent has to be of the highest possible level. That's where oracles come into play. Oracles in the future are going to be able to provide agents with streamlined, very efficient data pipelines and make sure they can really operate on something that is true. Outside of that, we see quite a lot of overlap when it comes to using the AI technology for oracles specifically. For example, for enriching different types of data in the future as well. So Clara is just the first implementation of what we want to do in the future between oracles and AI. Now, when it comes to how agents collaborate, now it's quite tricky and the communication layer is basically non-existent at this point. That's the missing link for building something that resembles a digital on-chain based economy. So specifically, if you want to do specializations of agents and you need to do it if you want to have the best agents in each category, then you need a very robust layer of communication between them. So let's say you would like to have one agent that's just going to be specializing in text-based interface based on GPT-4, for example. And then for analyzing videos, you would have a different model or different agent. And for analyzing audio, you'd have something based on whisper, for example. These are going to be different agents, different models as well. And if you want them to collaborate, you need to have them communicate with each other in machine-readable form. Aggregation of insights across multiple models. So basically the same mechanism as I've just described. There is going to be not only different agents for different things, but also different models specializing in different things. The general purpose LLMs that we've got right now, this is probably just a transitionary stage to something that's really going to be specialized. And you are going to have probably the best model per each category. Now syndication of expensive computation costs. Some of the computation required for the functioning of agents is just going to be heavy. And you are going to be able to spread the costs across multiple participants for everybody to be able to participate. For that you also need a specified layer for communication. Collective content generation. If you know how Truth Terminal started, this was basically a couple of agents talking with each other, or a couple of models talking with each other. And if you want to have that kind of a collective thinking, you also need a communication layer between the agents to be able to do it. None of that is currently in existence. Enter Clara. So a communication layer for agents specifically that basically has two main components. The first one is communication and exchanging of information or arbitrary logic between agents. But the second part, which I believe is even more important, is the economic layer. So agents can now transact with each other. And this means agents are now able to do commerce, which is basically the cornerstone of the existence of the digital agent-based economy that we want to build here. We've got two implementations of Clara. One on AO, one on Story Protocol. And this is just the early innings of it. This is still early experiment, but this is already on production. In the future, what we want to add to Clara is definitely things like privacy. Let's say you are a prop trading firm. You've got your own algorithm for trading. You've got an agent that does decisions for you as well. You probably do not want to disclose the details of your algorithm to the other agent that you would like to trade with. So that's also coming. Some heavy inscription that we are going to be implementing to Clara as well. But for now, the two main components are the most important. First one, communication between agents. Second one, the exchange of value that is now enabled by this cross-chain, cross-agent layer. And Clara also works for basically every single major framework that you have right now for deploying agents. Which means that around 10,000 agents that we currently have on-chain are now able to interact with Clara, are now able to communicate with each other, exchange arbitrary logic and exchange value. Which we believe is going to be the cornerstone for the digital agent-based economy that's still yet to come. Thank you so much. If you wanted to learn more, you can follow us on Twitter. There is quite big news coming, probably before the end of February. So be sure to follow us. There are new updates on Clara, are also going to be published there. Outside of that, I would like to wish you a fruitful rest of that conference and see you later at the venue. Thank you so much.