 Hello everyone, thanks for getting me up here to talk about a generational shift and Ethereum consensus that we are trying to push called Beam Consensus, Beam Chain. I'm Gajendra and I'm founder of Xeem and a co-developer and I also contribute to Lordstar and EthereumJS. So we will reflect on current Ethereum consensus and then we'll talk about why Beam and then we'll do a little bit deep dive into the basic fundamentals and mechanisms. So why do we need Beam? I mean everything is going so good on Ethereum network but when Ethereum transition from proof-of-work to proof-of-stake it opened up various avenues for scalability and decentralization. So execution layer became an API interface for decentralized apps and the new consensus layer it became a back-end with this responsibility to sequence the blocks. So what really happens is that when a new slot starts a block is minted and that is imported by other validators then they do attestations on it then those attestations are aggregated and then they are collated and then the new proposal for the next slot sort of includes them into the new block. With Casper FFG, Fast Finality Gadget, we have finality because all these validators they are voting on what they see as the canonical chain and with slashing mechanisms that we have on conflicting voting as well as conflicting broke proposal we get security and with randomly distributed proposal rights among a large and diverse validators that we get censorship resistance and it has been pretty successful we have like 1 million plus validators on Ethereum network and we have also got client diversity we have 5 plus consensus clients we have 5 plus execution clients and if you are a validator I would recommend you try loadstar because I contribute to it So now let's look at Ethereum roadmap so as you can see we have a lot of features that are jam-packed for execution data and consensus but for now let's just focus on consensus part of it which this presentation is all about So for scalability we have peer-to-peer-dias and we have big blocks which are targeted by fossil so that we stay censorship resistant then we have payload separation by EPBS we have a specification of engine and validator APIs so that clients can process blocks faster With the depository snapshot and for 4444 you will be able to now consume less resources for running your node and with the specification and the Proveable web 3.0 and provable web 3.0 initiatives in conjunction with light current protocol you basically don't need a node to verify what's going on in Ethereum so that gives us universal verifiability For latency we have pre confirmations but this is an extra protocol feature so your transaction is basically confirmed as soon as you send it rather than wait for 12 seconds for inclusion and then we have proposals like orbit single slot finality which will basically finalize the chain in that particular slot itself Right now there are no concrete proposals for dealing with quantum resistance And as you can see the rollout plan is pretty jam-packed and we are not sure whether all of the things that we have been talking about here will go through The reason for that is that rolling out these features on a production chain they become exponentially hard fork by fork And that's why we need Beam to have a relook at the consensus mechanism of Ethereum on with first principles Because zero knowledge technologies now provide this another unique opportunity to disrupt the consensus mechanism at very fundamental levels So with APS and SNARK-Files we can go for scalability we will have three second slot For universal verifiability again SNARK-Files with SNARK-Files transitions you can prove not only the block but then the canonicality of the entire chain itself on a device like mobile And with fast ZK aggregation we will even have one each validators For latency obviously three second slot addresses it and then for finality we will have three slot finality because all validators are voting on each of the slot And using again CASPER-FG we can achieve a three slot finality And most importantly for quantum resistance we have hash-based signatures that are proposed and they will be aggregated using post-quantum SNARKs We will get rid of VPOX, sync committee, deposit contract and other tech debt that has been accumulated in the current consensus layer But we will retain SSC because it has proven to be an efficient and versatile standard and implementation And its mercilization is also going to be quantum resistant Beam specs will be out by end of this year and hopefully we will be able to fork on mainnet by 2028 That is the plan But for now let's take a deeper look into some of the components Let's start with signatures So when producer creates a block they sign it and then validators import the block they sign their attestations or their votes And then those votes are aggregated by the aggregators before they are collated by the block producing the next block Beacon uses BLS crypto which is very easy to aggregate and it's very efficient While Beam proposes XMSS based one time written is quantum resistant signatures And for the aggregation we will need PQ SNARKs And the benchmarks are quite optimistic and I think they are I think Beam these signatures are ready to be used in a production setting So we can go into details of how these signatures work But only if we have time But what is the main thing to note about is that the hash-based signatures will require aggregation by post-quantum SNARKs And that is something that the Beam research is currently working on Now moving to the next part Let's take a deeper dive into state transition How that will work So when block producer assembles a block they basically apply it to their current state and they get a post-state Which is what we call state transition And then they embed the post-state route into the block So before validators import the block they have to verify that they executed the block correctly And then they basically add that block into their block tree and then they sign it So on the So Beam state transitions are going to be a lot simpler because there will be no epochs And but there is another They are also going to be a bit more involved because there is another element over here which is of state proving State transition proving So let's look at that pipeline So when producer creates a block they apply it They run state transition function and then they transmit the block while the validators they will They will basically again run and evaluate the block on their end But the producer can also optionally start proving the block and then they are third party that can also start proving the block Now if they take too long to prove the block then basically attestors can't really rely on it And they will have to run and evaluate the block themselves But if they do real time proving if they are able to prove the block pretty fast then the attestors don't need to run the block themselves They can just check the state transition verification proof verification and then they can just sign the attestations And as it turns out beam state being proving won't be that hard even main net blocks will be proved in a single slot by end of this year And so beam state transitions will be real time And this brings us to a very new interesting scenario where we have a very new situation We have a new entrance into the ecosystem where there are multi proofs that are going on These third parties they can use different kind of ZKVMs that are out there They can see those proofs into the ecosystem And that brings us to the new dimension of having proving diversity Where now we not only have CLs and ELs but we also have ZKVMs into the mix And also not to mention the new beam clients like ZIM and RIM And they are a work in progress And that will basically transition Ethereum to a new era of ZKEthereum Which will be very useful and hopefully pretty fast for its users Thank you I'll probably need less time so I'll just Hello Yes, these things are All righty Hello everybody, my name is Michael Ferris I'm a software engineer with the Ethereum Foundation I'm here today to talk about Portal Network I've been a software engineer with the Ethereum Foundation since 2021 And I'm really excited to talk about the Portal Network Because I've been working on it since 2021 And when I started working on it, it was specs It was theoretical, it didn't really exist in real life Now, today, really excited to talk to you guys about it because it exists So I'm going to explain to you what it is, what problem it solves We are solving a problem which is really exciting, that's a fun thing to do How it works and the most exciting part, how you can contribute very easily And it does say software engineer there, it does not say graphic designer That will come into play here, here we go So, just basics, if you want to access Ethereum, you want to access some block In the blockchain, you run a full node, you download the entire chain, you sync the whole thing It takes on the order of a terabyte in a day to sync the entire thing And when you're done, you can use the JSONRPC API to arbitrarily access any of that data within there The problem is that, and this has been known for a long time, I mean, Ethereum grows quickly And so we've known for a long time that eventually this is going to become unsustainable And clients are not going to, nobody's going to want to run a full node If it takes 4 terabytes, 8 terabytes, 16 terabytes, so on A lot of costs to that, a lot of knowledge required, a lot of, for a while it's been, what are we going to do about this? Full nodes are becoming inaccessible The answer, EIP 4444, AKA 4-4s And the idea is that, eventually, full nodes just stop storing the entire data And in March-ish, they're going to stop storing anything pre-merge So you can see here from Vitalik's grand plan You have the EIP 444 implementation and portal happening in parallel with each other That's a bit of a spoiler So we're going to stop, we're going to stop serving, on full nodes We're going to stop having this data accessible Obviously, we still need this data If you want to know what the state of the chain was at block 2 It's going to be a problem if all the nodes drop the data Where is that data going to live? The answer, I spoiled it, I'm sorry Portal network And so what is Portal? I mean, got a definition right here A network of resource-constrained machines working together to store and serve the entire validated Ethereum chain You can kind of think of it like, imagine all of us in this room, we all have the same problem We're all like, damn, we want to run a full node We each want to sync a full node, we each need that data None of us have the, maybe we don't have the resources or the desire or the time to sync that data Just to get access to it And we just sit there with our problem Unless we talk to each other, we come up with a solution like, hey, what if I store part of the data You store another part of the data, you store another part of the data And then we mutually can answer each other's request for whichever part of the data we are all storing And together we can kind of add up to this virtual full node in the abstract We can combine our powers to be a full node over a globally distributed full node And so, it's, Portal is, like it says here, sustainable through mutualism What I mean by that is that if you are using Portal, you're running a Portal node In order to consume the data on the Portal network, you are also storing data And other people are asking you for that data And so there's no kind of parasitic relationship For example, with, there was something called Light Ethereum Service, LES And that was a system where a full node could, anybody could come to a full node and say, hey, I don't have the, I'm a light client I don't have the resources to access this data, can I have block 12,992,462, whatever And there, I can't offer, if I'm asking you for that, there's nothing I can offer you I'm just saying, hey, can I have that data? And you're saying yes Or maybe you go through a centralized service like Infura and you pay them money And they can, you pay them money and they can rate limit you And all the, all the bad, I mean, this is, we're talking about Ethereum, we're talking about crypto The point is to be decentralized, get, not have these, not have any walls And so we want to avoid centralized services And avoid just asking a server, hey, can I have the data? With nothing to offer that server And eventually that server might say, whoever's running that server might say, wait a minute I'm getting nothing out of this, I'm going to stop offering the data So the portal network is, all of the requests within it are mutual If I'm, like, again, if I'm storing, if I'm asking for data from the portal network I'm also storing data on my machine And the way it works, when I run a client, I can configure how much of my machine I want to offer I can offer a kilobyte And so the network will give me about a kilobyte of data And anybody can come ask me for that data And, like it says, I can store up to a terab, on the order of terabytes And so it's completely configurable I can offer as much resource as I have, or as little And still be able to use the portal network How does it work implemented under the hood? It is Kdemlia This is an algorithm for implementing what's called a distributed hash table A hash table is just a key value store where I say, hey, I want Like I said, I'm not going to say it again, block 12 million, whatever, 900 And that maps to a value which contains the actual value The Kdemlia paper is a very interesting one I'd recommend anybody read it, it's short It's good if you want to work on your attention span and actually read things Like we all need to do, it's a good paper to read It contains a little, it's mainly geared toward It's written specifically for building a distributed hash table Such that you can find other nodes in the network It has a little blurb about how you can extend that To store content on those nodes And be able to find content within that network And we can't, Portal is basically taking that little blurb in the Kdemlia paper About being able to store content and saying, hmm And running very far with it And now storing terabytes and terabytes of data in this distributed System, it has desirable properties like You want, you don't want to scale linearly with the number of nodes in the network It scales logarithmically And so it can Access, times, scale in favorable ways As opposed to getting, as opposed to not being able to scale globally We're talking about building a globally decentralized virtual full node It needs to be able to scale Kdemlia has all those properties And then how does it work even further? It's actually layers of Kdemlia networks There are portal subnetworks The portal network is made up of subnetworks within it There's the beacon network, history subnetwork, and the state subnetwork And the arrows here refer to the fact that So when you request data out of the portal network Obviously you, without having synced the chain The point of syncing the chain is to know that any data you're getting Is cryptographically verifiable by the block before Which is cryptographically verifiable by the block before If you're just getting data willy-nilly, you can't do that And so what you do instead is we have the We can use the, with proof of stake It introduced light client, a light client architecture That allows us to validate execution blocks Against the beacon, against the beacon chain And so there's a, basically you can optionally run portal subnetworks based on your needs If you only need beacon data, you can just run the beacon subnetwork If you need history data, like, which is headers, blocks, and receipts You would run the beacon network, which you could use to validate data coming out of the history network And then, taking it a step further, if you need state data, like what is your wallet balance Or your account, an account balance at some block, you would run the beacon network, which you would use to validate the data you're getting out of the history network, which you would then use to validate the data you're getting out of the state network So it's this stack of portal subnetworks that make up the portal network in its entirety And allow you to, one, optionally run whatever you need It's kind of a, you run according to your minimum needs And not need to, for example, store a bunch of state data, which is the largest data set within Ethereum It's about 20 terabytes in its entirety You don't, if you're not, if you don't want state data, you don't need to store it Going back to the mutualism that we talked about earlier Where whatever you are requesting from the network, you are also storing and giving to others So it's mutual and modular and And so, taking that modularity a step further You can break up, the idea is to break up an entire execution client into These separate portal networks We have a couple other portal networks here The portal transaction mempool and portal index Do not exist yet But in there, those together with history and state could make up Will make up an entire execution client, which is the roadmap going forward To be specific, to be specific on something I mentioned earlier For implementing 4.4s That's just the portal history network And so that is implemented And we have all history data stored within there And then you might be wondering, okay, you can get the data out of this But where is this data coming from? It takes a few, it only takes a few full nodes Altruistically, putting their data into the network For that data to then be gossiped around the network And saturate the network so that all the nodes that should have that data Where all the nodes that will, are where that data will be looked for, will have it And this is in contrast to, like I mentioned earlier, with the LES, Light Ethereum I mean, imagine if, kind of the, to use that analogy again If everybody in this room wanted data but didn't want to sink an entire full node And I'm the full node And all of you keep coming to me and saying, hey, can I have this? Hey, can I have this? And you're all asking for the same data Eventually I'm going to be like, this is not sustainable Everybody's just asking me, every, every person that needs a piece of data needs to ask me for it This model, how it instead works is that One of you comes and asks me for a piece of data I basically say, okay, here's that piece of data Don't ask me for that again You now go tell your friends about it Spread that around so that I don't have to do the work of having somebody come request that from me again So that's why I say a small number of full nodes It's scalable for these, what are called bridge nodes You need a couple of, I mean, I won't say a couple, there's not an exact number But it is a relatively small number because of that model, because of that only needing to bridge it once And then it moves around the network on its own It does a lot of work for the full node And so it takes only a few full nodes to bridge data into the network and have the network be fully populated I don't, I didn't mean to sponsor this by Coca-Cola, I forgot I had this in my hand And then the, obviously there's trade-offs, if you're hearing me say like, you can access data arbitrarily We don't need full nodes anymore, we still need full nodes for plenty of use cases The trade-off is latency If you're jumping around the network looking for each piece of data, especially if you're doing state access You need to jump down the tree, the state tree You can get into the, on the order of a second for accessing state Or a few seconds for accessing state, at the moment obviously we want to do optimizations But obviously this is, you can't like, just the nature of the universe, there's trade-offs And the trade-off is, portal is going to be slower obviously than just having it on disk But you don't need to, if you don't want to, or can't store it on your disk, portal is going to be for you So, going back to that modularity, and the stack of portal sub-networks that the portal network is made up of You can choose how you participate in the, in the Ethereum protocol And so I work on Trin, it's a Rust implementation written by some other engineers and I at the Ethereum Foundation There's a Rust implementation, there is a, well, I'm kind of, these are the people that are writing the clients Status is a company that's writing a Fluffy client, there's open source ones, you maybe, if you want to get involved I'm going to put a lot of links at the end, so you know what's going on So I work on the Rust, Trin, Rust implementation called Trin There's a Nim implementation called Fluffy, Go implementation called Shisui, which excitingly is in the works to be integrated into Geth And then there's Java implementation, a C-sharp implementation that's being implemented, integrated into Bezu And so that's going to be great because going back to what I said about the bridges, we need those full nodes And so it'll be great if full nodes automatically can, out of the box, bridge data into the network And then also those full nodes, like Go, Geth, will be using this, like if you request block one, it'll be using portal to go and find that block one And so yeah, just want to emphasize, like I said, a lot of times in the past, we've talked about portal network and it's this thing where it's like, yeah, this is coming, eventually And so I just really want to emphasize that it is here, we have all of our, this is a dashboard that we run to monitor And we, the audit success is really boring at the bottom because it's 100%, we have 4-4's data in the network and accessible This is kind of, if you're a visual person, and didn't quite grasp the Kdemlia stuff I was talking about Down here you can kind of see, this is jumping around the network, and then there's a, node IDs and content IDs are in the same key space And you can kind of see starting at the orange dot, it jumps and go, and basically inches closer to where it wants to go In the network and eventually finds it at the green dot We have a Trin desktop, this is like I mentioned at the beginning, you can actually contribute to this, which is really exciting If you download Trin desktop, you can run it on your machine, and that will actually just run in the background a portal client Which will receive data, you can configure how much data you want to offer to the network, like I said, you can go over the network It will go up or down, and it will offer to the network as much data, and allow you to get data from the network And so here you can see us doing an eth-get block by number, and successfully retrieving blocks That does require state, which is more of a coming in Q2 type of thing So if this doesn't, this will work around block 21 million, and we are seeding the rest into the network And now anybody can access the Ethereum protocol, and here is where you should find out more You can come into our Discord, ask any questions you want, and we have a website, Twitter, the specs And yeah, we are really excited about it, and I am excited that you guys can contribute with no, really no effort Which is kind of fun, thank you Thank you Thank you Should I begin? Okay . Should I begin? OK. Hello. My name is Jihoon. Today, I want to talk about inclusion list. In this talk, I'd like to aim to deliver intuitions behind inclusion list, what problem it aims to solve, how it works, basically, what could help understand what we are trying to achieve with inclusion list and why. In Ethereum, we propose a block and link it to the chain in each slot. In this talk, we have a group of people who stake some amount of it to become block proposers. This proposal is selected in each slot, proposes a block, and gets the rewards. And this continues. In the next slot, another proposal is selected, proposes a block, and gets the rewards. However, it was not before long that people realized they could get extra rewards by ordering transactions. It was a lucrative business, so more and more people became sophisticated and centralized for bigger profits. To mitigate this centralization risk, an off-the-protocol market was emerged. We still select one proposer at a time, but instead of constructing a block by themselves, the proposer now delegates block construction to a specialized set of entities called builders. Builders participate in the auction, and the bid with the most valuable execution payload wins the auction. The proposer includes that execution payload in a block, gets the rewards, and the builder shares in the extra profits. We call this collaboration the proposer-builder separation. Now, proposers could remain unsophisticated, but the centralizing force within proposers has shifted to builders. Builders reinvested the rewards to become more sophisticated and centralized for leveraging the economies of scale. Today, more than 90% of blocks are produced by a handful of entities, and they now have the power to not include certain transactions for some reason, whether it be regulatory, economic, or strategic. This censorship is actually happening at the moment, and if your transaction is being censored, you have little choice but to wait until you meet non-censoring entities. How can we handle this? Can we specify certain transactions and tell builders to include them? And can we enforce it by not accepting a block if it doesn't include any of those transactions? This is the idea behind the inclusion list. In the context of inclusion list, we aim to preserve censorship resistance by providing a channel for a diverse set of participants to have a say in the block content. There have been many ideas and designs for inclusion list proposed for the past two years, and Fossil, fork choice enforced inclusion list, is the most agreed upon design. Fossil has a simple yet effective solution. We have proposers and we select one proposer at a time. The proposer delegates block construction to builders and builders work on their payload as usual. We now introduce inclusion list committee. Each inclusion list committee member constructs and broadcasts inclusion lists over the P2P network. Builders should listen to those inclusion lists, and update their payload to add any missing IL transactions. The winning bid gets included in a block and the block is now in the hands of our testers. Our testers check if all IL transactions are included in a block and accept the block only if the check passes. The block would become canonical and the builder and proposer will get their rewards. This ensures ethereum accepts block only if it satisfies inclusion list compliant and rejects it otherwise. This is the goal of the project. So that's how Fossil works in a nutshell. And now let's get into the detail by taking a look into them one by one. Let's suppose that we have 6% solo stakers or diverse set of participants. IL committee has 16 members so we have roughly 63% chance to have at least one solo staker in each IL committee. We currently don't have incentive mechanism for inclusion list committee members so Fossil assumes there will be at least one honest member and will rely on altruistic behavior. We will get back to this later. And before Fossil, it takes roughly 17 slots for any solo stakers to become a block proposer. But with Fossil, a solo staker is expected to become an IL committee member by every roughly two slots. This means with Fossil, solo stakers can now signal what should be included in a block more than 10 times than before, which is a significant improvement in censorship resistance. So this was about inclusion list creation. Now let's talk about inclusion list enforcement. A testers accept the block only if it includes all IIN transactions. But builders can omit IL transaction if it's invalid or when a block is full. So when the network is congested, builders can prioritize their transactions over IL transactions. Also we don't impose any ordering rule, so builders have a freedom to place IL transactions however they want. We don't want to allow to use inclusion list as a vehicle for MEV. And this could prevent inclusion list from degenerating from its original purpose. So we can expect one inclusion list to carry roughly 40 transactions. All inclusion lists become public and there's enough time for everyone to learn about them. So it's not a good idea for you to put something valuable in there that no one else is aware of. We do want to use inclusion list for censorship resistant purpose only. And all of these are the reasons why we believe the friction to create the off the protocol inclusion list market would be bigger than expected revenue. The intuition is if you were to pay inclusion list member to force include your transaction for some reason, you'd want to talk to proposal directly or you don't even need to pay because there will be at least one honest member who will include your transaction anyway. Fossil has some active areas of research. Firstly, we are doing research on the mechanism. Network participants usually act honestly and having only one honest member is enough to keep Fossil running. But having its own incentive mechanism would help us stop relying on altruistic behavior, making it more robust. It also doesn't support block transactions. So we need a way to easily verify block data availability. Last but not least, we want to preserve privacy of the IL committee members. Some IL transaction might be considered sensitive for some reason. So we are exploring cryptographic methods to unlink who proposed which inclusion list. Fossil is currently proposed for inclusion in Fusaka, which is the upgrade after Pectra. And all client teams are busy with Pectra at the moment. But we have already seven client teams making a good progress. Imagine Ethereum can't censor instead of don't censor. For more information, please check the QR codes to the EIP and our website. And thank you for your listening.