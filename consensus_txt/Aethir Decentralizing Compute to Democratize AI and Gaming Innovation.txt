 I think she introduces our project better than I will. But great to be here. My name is Mark. I'm one of the co-founders here at Aether. We build distributed GPU infrastructure for gaming and AI companies. You can think of us like a Google Cloud or an AWS, except we focus exclusively on GPUs, so not storage, not CPUs, and we don't actually own any of the GPUs in our infrastructure. We're kind of like an Airbnb for enterprise-grade, high-performance compute infrastructure. So I think the best way to understand what we do is to take a little bit of a look at the problem that we address. You may have seen celebrities like Elon Musk come out and say things like, GPUs are harder to buy than drugs. He then goes and spends a whole bunch of money on 100,000 H100s in a decommissioned facility in Tennessee just to support his own incumbent frontier AI model, Grok. By the way, that had a pretty cool release just a couple of days ago. If you haven't checked out Grok 3 on Twitter, definitely recommend checking it out. You have Mark Zuckerberg out there buying $11 billion worth of GPUs, knowing that the next version of those GPUs is coming out just a few months later. Even, you know, OpenAI, right? Raising the largest venture rounds in history just so they can get access to the GPUs necessary to power their models. This is putting a strange amount of pressure on the ecosystem, especially considering GPUs are obviously one of the key pillars of what you need to build AI. At Asa, we took a little bit of a look at this problem, and we broke it down into really four key pieces. First of all, of course, is by pressure. You have the biggest companies in the world with the deepest pockets, essentially with an insatiable appetite for this compute. The more compute that they have, the faster they can train their models, the more likely they are to win the AI race. Adjacent to that, of course, is the production limits, right? The vast, vast majority of the chips required, the vast majority of the GPUs required to power AI are produced by NVIDIA, and they can only produce so many per year. And right now, the amount that they can produce is not yet enough to match the demand in the market and may actually never be. And that's where kind of the third point comes in. What we're seeing is this increase in LLM complexity and competence, right? But as that competence increases, we're also seeing essentially an exponential cost increase in training, right? So it's exponentially more expensive to train ChatGPT4 than ChatGPT3, right? And the same is expected for each ongoing frontier model as we progress up this kind of competence curve. Unfortunately, even though the cost, the performance cost of training these new models is exponentially increasing, we're not seeing an exponential increase in the performance of GPUs within the same time period. We're not seeing an exponential increase in the availability of supply within the same time period. So there seems to be this widening gap between the available compute out there in the market and the requirements of these future stage models. I think, interestingly, DeepSeq challenges that narrative just slightly. It did show that some post-production kind of modification on the inferencing side does give smaller models the ability to compete with these bigger ones. But I don't think anyone actually sees that as maybe a path towards AGI, which is really the main competition that these big frontier models are addressing. But the final challenge here, and the one that I think Aether addresses most, is the inefficiency of the management of the resources that do exist, right? You have a large amount of these resources being bought up by big companies, by telcos, by companies that can access these resources, and they end up often sitting in data centers unavailable for use because maybe the team only realized that they needed 80% of the supply or they had a production delay. And this is incredibly expensive equipment that just sits idle, burning a hole in companies' pockets. So effectively what Aether has done is create this infrastructure that allows this idle, high-performance GPU compute to be resurfaced into an infrastructure layer that allows us to match that compute with global demand. So essentially Aether is this distributed enterprise-grade GPU cloud infrastructure that aggregates global high-performance compute, redistributing it all over the world, supporting builders in the AI and also gaming space, although I'm not really touching on the gaming side today. This is a quick look at our stack. What we do, you can see on the top right there we have AI training. This is the process of taking AI from ChatGPT3 to ChatGPT4. This is built on the back of H100, H200 infrastructure. We have the largest collection of these GPUs in Web3, well over 5,000, 6,000 of them. On the right-hand side you have AI inference. This is the process of AI doing its job. You've trained the AI, now you're asking it to answer questions, you're asking it to execute if you're talking to an agent, for example. All of that also happens on the back of GPUs, and that is called inferencing. A couple of highlights. Main one here is our $100 million ecosystem fund. We're actively looking to support builders in the AI agent space. We have a large amount of GPU credits that we're looking to deploy alongside investment. We've invested in around 30 different startups over the last kind of two months, and we're just interested in supporting projects that will be building on top of the infrastructure that we're deploying globally now. But thank you very much.